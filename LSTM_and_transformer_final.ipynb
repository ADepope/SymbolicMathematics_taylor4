{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from tokenizers.normalizers import NFKC\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=32, hidden_dim=32, num_layers=2, dropout=0.5):\n",
    "        super(Seq2SeqLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src, tgt = self.embedding(src), self.embedding(tgt)\n",
    "        _, (hidden, mem_cell) = self.encoder_lstm(src)\n",
    "        output, _ = self.decoder_lstm(tgt, (hidden, mem_cell))\n",
    "        return self.linear(output)\n",
    "    \n",
    "def encode_sequence(text, tokenizer):\n",
    "    return torch.tensor(tokenizer.encode(text).ids, dtype=torch.long)\n",
    "\n",
    "def pad_sequences(sequences, pad_token_id):\n",
    "    return pad_sequence(sequences, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "def train_lstm(model, train_loader, val_loader, epochs, criterion, optimizer, device, model_path=\"best_lstm_model.pth\", patience=20):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_improvement = 0  \n",
    "\n",
    "    train_losses = []  # training loss\n",
    "    val_losses = []    # validation loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(\"..early stopping triggered.\") # too much time with no imporvement\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), tgt_output.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss) \n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "                outputs = model(src, tgt_input)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), tgt_output.contiguous().view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss) \n",
    "\n",
    "        print(f\"..epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"..best model saved with val loss = {best_val_loss}!\")\n",
    "            epochs_since_improvement = 0  \n",
    "        else:\n",
    "            epochs_since_improvement += 1  \n",
    "\n",
    "    return model_path, train_losses, val_losses\n",
    "\n",
    "def evaluate_lstm(model, test_loader, device, criterion, vocab_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in test_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), tgt_output.contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader):.4f}\")\n",
    "\n",
    "def plot_loss(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Validation Loss\", marker='o')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Train and val loss over epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "..vocabulary size is 70\n"
     ]
    }
   ],
   "source": [
    "# we first load in the expressions in prefix form and train the tokenizer which we then use to encode the prefix expresseion into a sequence of ints\n",
    "file_path = \"/Users/adepope/Documents/symbolic_py/data.prefix.expressions\"\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['f', 'taylor_f'], engine='python', nrows=10000)\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"])\n",
    "tokenizer.train_from_iterator(df[\"f\"].tolist() + df[\"taylor_f\"].tolist(), trainer)\n",
    "\n",
    "df[\"f_tokens\"] = df[\"f\"].apply(lambda x: encode_sequence(x, tokenizer))\n",
    "df[\"taylor_f_tokens\"] = df[\"taylor_f\"].apply(lambda x: encode_sequence(x, tokenizer))\n",
    "\n",
    "pad_token_id = tokenizer.token_to_id(\"<PAD>\")\n",
    "padded_f_tokens = pad_sequences(df[\"f_tokens\"].tolist(), pad_token_id)\n",
    "padded_taylor_f_tokens = pad_sequences(df[\"taylor_f_tokens\"].tolist(), pad_token_id)\n",
    "\n",
    "# train / validation / test split of the 10k samples we loaded\n",
    "train_f, test_f, train_taylor_f, test_taylor_f = train_test_split(padded_f_tokens, padded_taylor_f_tokens, test_size=0.2, random_state=42)\n",
    "train_f, val_f, train_taylor_f, val_taylor_f = train_test_split(train_f, train_taylor_f, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = TensorDataset(train_f, train_taylor_f), TensorDataset(val_f, val_taylor_f), TensorDataset(test_f, test_taylor_f)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"..vocabulary size is {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluation\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # working locally on mac, change to 'gpu' if working on a HPC\n",
    "model = Seq2SeqLSTM(vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "num_epochs = 256\n",
    "best_model_path, train_losses, val_losses = train_lstm(model, train_loader, val_loader, num_epochs, criterion, optimizer, device)\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "evaluate_lstm(model, test_loader, device, criterion, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_loss(\u001b[43mtrain_losses\u001b[49m, val_losses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one example from the test set - teacher inference\n",
    "best_model_path = \"best_lstm_model.pth\"\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "example_idx = 64\n",
    "input_function = test_df.iloc[example_idx][\"f\"]\n",
    "target_value = test_df.iloc[example_idx][\"taylor_f\"]\n",
    "\n",
    "encoded_input = encode_sequence(input_function, tokenizer)\n",
    "encoded_input = encoded_input.unsqueeze(0).to(device)\n",
    "encoded_target = encode_sequence(target_value, tokenizer).unsqueeze(0).to(device)\n",
    "\n",
    "tgt_input = encoded_target[:, :-1]\n",
    "tgt_output = encoded_target[:, 1:]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(encoded_input, tgt_input)\n",
    "    loss = criterion(outputs.view(-1, outputs.size(-1)), tgt_output.contiguous().view(-1))\n",
    "\n",
    "predicted_ids = outputs.argmax(dim=-1)[0].cpu().numpy()\n",
    "predicted_output = tokenizer.decode(predicted_ids)\n",
    "\n",
    "print(f\"Input Function: {input_function}\")\n",
    "print(f\"Predicted Expression: {predicted_output}\")\n",
    "print(f\"Target Expression: {target_value}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=2, dim_feedforward=1024, max_seq_len=512, dropout=0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, max_seq_len, d_model))\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_layers, \n",
    "            num_decoder_layers=num_layers, \n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "\n",
    "        output = self.transformer(src_emb, tgt_emb)\n",
    "        return self.output_layer(output)\n",
    "    \n",
    "def encode_sequence(sequence, tokenizer, max_len=512):\n",
    "    tokens = tokenizer.encode(sequence).ids\n",
    "    tokens = tokens[:max_len] + [0] * (max_len - len(tokens))\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "def train_transformer(model, train_loader, val_loader, epochs, criterion, optimizer, device, save_path, clip_grad=1.0):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), tgt_output.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = evaluate_transformer(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Best model saved at epoch {epoch+1} with validation loss {best_val_loss:.4f}\")\n",
    "\n",
    "def evaluate_transformer(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), tgt_output.contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"/Users/adepope/Documents/symbolic_py/data.prefix.expressions\"\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['f', 'taylor_f'], engine='python', nrows=10000)\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"])\n",
    "tokenizer.train_from_iterator(df[\"f\"].tolist() + df[\"taylor_f\"].tolist(), trainer)\n",
    "\n",
    "df[\"f_tokens\"] = df[\"f\"].apply(lambda x: encode_sequence(x, tokenizer))\n",
    "df[\"taylor_f_tokens\"] = df[\"taylor_f\"].apply(lambda x: encode_sequence(x, tokenizer))\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.stack(list(train_df[\"f_tokens\"])), \n",
    "    torch.stack(list(train_df[\"taylor_f_tokens\"]))\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.stack(list(val_df[\"f_tokens\"])), \n",
    "    torch.stack(list(val_df[\"taylor_f_tokens\"]))\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.stack(list(test_df[\"f_tokens\"])), \n",
    "    torch.stack(list(test_df[\"taylor_f_tokens\"]))\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = Seq2SeqTransformer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_model_path = \"best_transformer_model_taylor4.pth\"\n",
    "\n",
    "train_transformer(model, train_loader, val_loader, epochs, criterion, optimizer, device, best_model_path)\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_loss = evaluate_transformer(model, test_loader, criterion, device)\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"best_transformer_model.pth\"\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "example_idx = 9\n",
    "input_function = test_df.iloc[example_idx][\"f\"]\n",
    "target_value = test_df.iloc[example_idx][\"taylor_f\"]\n",
    "\n",
    "encoded_input = encode_sequence(input_function, tokenizer)\n",
    "encoded_input = encoded_input.unsqueeze(0).to(device)\n",
    "encoded_target = encode_sequence(target_value, tokenizer).unsqueeze(0).to(device)\n",
    "\n",
    "tgt_input = encoded_target[:, :-1]\n",
    "tgt_output = encoded_target[:, 1:]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(encoded_input, tgt_input)\n",
    "    loss = criterion(outputs.view(-1, outputs.size(-1)), tgt_output.contiguous().view(-1))\n",
    "\n",
    "print(outputs)\n",
    "predicted_ids = outputs.argmax(dim=-1)[0].cpu().numpy()\n",
    "predicted_output = tokenizer.decode(predicted_ids)\n",
    "\n",
    "print(f\"Input Function: {input_function}\")\n",
    "print(f\"Predicted Expression: {predicted_output}\")\n",
    "print(f\"Target Expression: {target_value}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
