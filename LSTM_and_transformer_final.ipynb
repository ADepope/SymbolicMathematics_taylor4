{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from tokenizers.normalizers import NFKC\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate the data I follow the approach in Deep Learning for Symbolic Mathematics (https://doi.org/10.48550/arXiv.1912.01412). <br>\n",
    "I modified the code by adding gen_taylor_approx(self, rng) procedure inside /src/envs/char_sp.py. The output expression are in the prefix notation. <br>\n",
    "Additionally, there is a also a Mathematica code that generates random function in the rest of the GitHub repo. <br> <br>\n",
    "\n",
    "In the following part of the notebook we write up standard encode-decoder acrchitecture for LSTM and Transformer approach and examin test loss as well as performance on one example on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=512, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "        super(Seq2SeqLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id)\n",
    "        self.encoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src, tgt = self.embedding(src), self.embedding(tgt)\n",
    "        _, (hidden, mem_cell) = self.encoder_lstm(src)\n",
    "        output, _ = self.decoder_lstm(tgt, (hidden, mem_cell))\n",
    "        return self.linear(output)\n",
    "    \n",
    "def encode_sequence(text, tokenizer):\n",
    "    return torch.tensor(tokenizer.encode(text).ids, dtype=torch.long)\n",
    "\n",
    "def pad_sequences(sequences, pad_token_id):\n",
    "    return pad_sequence(sequences, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "def train_lstm(model, train_loader, val_loader, epochs, criterion, optimizer, device, model_path=\"best_lstm_model.pth\", patience=20):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_improvement = 0  \n",
    "\n",
    "    train_losses = []  # training loss\n",
    "    val_losses = []    # validation loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(\"..early stopping triggered.\") # too much time with no imporvement\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), tgt_output.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss) \n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "                outputs = model(src, tgt_input)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), tgt_output.contiguous().view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss) \n",
    "\n",
    "        print(f\"..epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"..best model saved with val loss = {best_val_loss}!\")\n",
    "            epochs_since_improvement = 0  \n",
    "        else:\n",
    "            epochs_since_improvement += 1  \n",
    "\n",
    "    return model_path, train_losses, val_losses\n",
    "\n",
    "def evaluate_lstm(model, test_loader, device, criterion, vocab_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in test_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), tgt_output.contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader):.4f}\")\n",
    "\n",
    "def plot_loss(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Validation Loss\", marker='o')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Train and val loss over epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "..vocabulary size is 70\n"
     ]
    }
   ],
   "source": [
    "# we first load in the expressions in prefix form and train the tokenizer which we then use to encode the prefix expresseion into a sequence of ints\n",
    "file_path = \"/Users/adepope/Documents/symbolic_py/data.prefix.expressions\"\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['f', 'taylor_f'], engine='python', nrows=10000)\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"])\n",
    "tokenizer.train_from_iterator(df[\"f\"].tolist() + df[\"taylor_f\"].tolist(), trainer)\n",
    "\n",
    "df[\"f_tokens\"] = df[\"f\"].apply(lambda x: encode_sequence(x, tokenizer))\n",
    "df[\"taylor_f_tokens\"] = df[\"taylor_f\"].apply(lambda x: encode_sequence(x, tokenizer))\n",
    "\n",
    "pad_token_id = tokenizer.token_to_id(\"<PAD>\")\n",
    "padded_f_tokens = pad_sequences(df[\"f_tokens\"].tolist(), pad_token_id)\n",
    "padded_taylor_f_tokens = pad_sequences(df[\"taylor_f_tokens\"].tolist(), pad_token_id)\n",
    "\n",
    "# train / validation / test split of the 10k samples we loaded\n",
    "train_f, test_f, train_taylor_f, test_taylor_f = train_test_split(padded_f_tokens, padded_taylor_f_tokens, test_size=0.2, random_state=42)\n",
    "train_f, val_f, train_taylor_f, val_taylor_f = train_test_split(train_f, train_taylor_f, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = TensorDataset(train_f, train_taylor_f), TensorDataset(val_f, val_taylor_f), TensorDataset(test_f, test_taylor_f)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"..vocabulary size is {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..epoch [1/256], Train Loss: 2.6141, Val Loss: 1.8136\n",
      "..best model saved with val loss = 1.8135580253601074!\n",
      "..epoch [2/256], Train Loss: 1.6440, Val Loss: 1.4858\n",
      "..best model saved with val loss = 1.485820460319519!\n",
      "..epoch [3/256], Train Loss: 1.4254, Val Loss: 1.3332\n",
      "..best model saved with val loss = 1.333234691619873!\n",
      "..epoch [4/256], Train Loss: 1.2988, Val Loss: 1.2308\n",
      "..best model saved with val loss = 1.2308308172225952!\n",
      "..epoch [5/256], Train Loss: 1.2132, Val Loss: 1.1599\n",
      "..best model saved with val loss = 1.1598756122589111!\n",
      "..epoch [6/256], Train Loss: 1.1496, Val Loss: 1.1065\n",
      "..best model saved with val loss = 1.1065454626083373!\n",
      "..epoch [7/256], Train Loss: 1.1001, Val Loss: 1.0599\n",
      "..best model saved with val loss = 1.0599498224258423!\n",
      "..epoch [8/256], Train Loss: 1.0560, Val Loss: 1.0250\n",
      "..best model saved with val loss = 1.0249543380737305!\n",
      "..epoch [9/256], Train Loss: 1.0216, Val Loss: 0.9937\n",
      "..best model saved with val loss = 0.9936593556404114!\n",
      "..epoch [10/256], Train Loss: 0.9953, Val Loss: 0.9664\n",
      "..best model saved with val loss = 0.9663843560218811!\n",
      "..epoch [11/256], Train Loss: 0.9674, Val Loss: 0.9443\n",
      "..best model saved with val loss = 0.9442897272109986!\n",
      "..epoch [12/256], Train Loss: 0.9467, Val Loss: 0.9269\n",
      "..best model saved with val loss = 0.926874783039093!\n",
      "..epoch [13/256], Train Loss: 0.9216, Val Loss: 0.9066\n",
      "..best model saved with val loss = 0.9065814042091369!\n",
      "..epoch [14/256], Train Loss: 0.9048, Val Loss: 0.8891\n",
      "..best model saved with val loss = 0.8890797472000123!\n",
      "..epoch [15/256], Train Loss: 0.8914, Val Loss: 0.8745\n",
      "..best model saved with val loss = 0.8745299029350281!\n",
      "..epoch [16/256], Train Loss: 0.8734, Val Loss: 0.8662\n",
      "..best model saved with val loss = 0.8662021684646607!\n",
      "..epoch [17/256], Train Loss: 0.8577, Val Loss: 0.8511\n",
      "..best model saved with val loss = 0.8511115741729737!\n",
      "..epoch [18/256], Train Loss: 0.8457, Val Loss: 0.8373\n",
      "..best model saved with val loss = 0.8373081827163696!\n",
      "..epoch [19/256], Train Loss: 0.8321, Val Loss: 0.8254\n",
      "..best model saved with val loss = 0.8253912830352783!\n",
      "..epoch [20/256], Train Loss: 0.8177, Val Loss: 0.8149\n",
      "..best model saved with val loss = 0.8149173665046692!\n",
      "..epoch [21/256], Train Loss: 0.8078, Val Loss: 0.8056\n",
      "..best model saved with val loss = 0.8055704545974731!\n",
      "..epoch [22/256], Train Loss: 0.7948, Val Loss: 0.7921\n",
      "..best model saved with val loss = 0.7921499562263489!\n",
      "..epoch [23/256], Train Loss: 0.7864, Val Loss: 0.7864\n",
      "..best model saved with val loss = 0.7864134454727173!\n",
      "..epoch [24/256], Train Loss: 0.7755, Val Loss: 0.7805\n",
      "..best model saved with val loss = 0.7805245447158814!\n",
      "..epoch [25/256], Train Loss: 0.7670, Val Loss: 0.7698\n",
      "..best model saved with val loss = 0.7698396229743958!\n",
      "..epoch [26/256], Train Loss: 0.7614, Val Loss: 0.7666\n",
      "..best model saved with val loss = 0.7665782260894776!\n",
      "..epoch [27/256], Train Loss: 0.7495, Val Loss: 0.7572\n",
      "..best model saved with val loss = 0.7571845889091492!\n",
      "..epoch [28/256], Train Loss: 0.7404, Val Loss: 0.7467\n",
      "..best model saved with val loss = 0.7467195510864257!\n",
      "..epoch [29/256], Train Loss: 0.7353, Val Loss: 0.7413\n",
      "..best model saved with val loss = 0.741284453868866!\n",
      "..epoch [30/256], Train Loss: 0.7250, Val Loss: 0.7366\n",
      "..best model saved with val loss = 0.7365523338317871!\n",
      "..epoch [31/256], Train Loss: 0.7168, Val Loss: 0.7320\n",
      "..best model saved with val loss = 0.7319719839096069!\n",
      "..epoch [32/256], Train Loss: 0.7120, Val Loss: 0.7254\n",
      "..best model saved with val loss = 0.7254034018516541!\n",
      "..epoch [33/256], Train Loss: 0.7064, Val Loss: 0.7203\n",
      "..best model saved with val loss = 0.7202568554878235!\n",
      "..epoch [34/256], Train Loss: 0.7003, Val Loss: 0.7128\n",
      "..best model saved with val loss = 0.7127599310874939!\n",
      "..epoch [35/256], Train Loss: 0.6932, Val Loss: 0.7089\n",
      "..best model saved with val loss = 0.7088937330245971!\n",
      "..epoch [36/256], Train Loss: 0.6858, Val Loss: 0.7072\n",
      "..best model saved with val loss = 0.7071753859519958!\n",
      "..epoch [37/256], Train Loss: 0.6801, Val Loss: 0.6990\n",
      "..best model saved with val loss = 0.699045114517212!\n",
      "..epoch [38/256], Train Loss: 0.6744, Val Loss: 0.6931\n",
      "..best model saved with val loss = 0.6931312274932862!\n",
      "..epoch [39/256], Train Loss: 0.6720, Val Loss: 0.6876\n",
      "..best model saved with val loss = 0.6875532889366149!\n",
      "..epoch [40/256], Train Loss: 0.6640, Val Loss: 0.6847\n",
      "..best model saved with val loss = 0.6846849918365479!\n",
      "..epoch [41/256], Train Loss: 0.6573, Val Loss: 0.6796\n",
      "..best model saved with val loss = 0.6796177387237549!\n",
      "..epoch [42/256], Train Loss: 0.6535, Val Loss: 0.6781\n",
      "..best model saved with val loss = 0.678051950931549!\n",
      "..epoch [43/256], Train Loss: 0.6499, Val Loss: 0.6736\n",
      "..best model saved with val loss = 0.6736219072341919!\n",
      "..epoch [44/256], Train Loss: 0.6395, Val Loss: 0.6662\n",
      "..best model saved with val loss = 0.6662147045135498!\n",
      "..epoch [45/256], Train Loss: 0.6396, Val Loss: 0.6643\n",
      "..best model saved with val loss = 0.6643374729156494!\n",
      "..epoch [46/256], Train Loss: 0.6306, Val Loss: 0.6613\n",
      "..best model saved with val loss = 0.6613491868972778!\n",
      "..epoch [47/256], Train Loss: 0.6261, Val Loss: 0.6583\n",
      "..best model saved with val loss = 0.6582994103431702!\n",
      "..epoch [48/256], Train Loss: 0.6255, Val Loss: 0.6547\n",
      "..best model saved with val loss = 0.6546624326705932!\n",
      "..epoch [49/256], Train Loss: 0.6162, Val Loss: 0.6486\n",
      "..best model saved with val loss = 0.6485820746421814!\n",
      "..epoch [50/256], Train Loss: 0.6161, Val Loss: 0.6497\n",
      "..epoch [51/256], Train Loss: 0.6087, Val Loss: 0.6433\n",
      "..best model saved with val loss = 0.6432849276065826!\n",
      "..epoch [52/256], Train Loss: 0.6032, Val Loss: 0.6445\n",
      "..epoch [53/256], Train Loss: 0.6007, Val Loss: 0.6355\n",
      "..best model saved with val loss = 0.6354800426959991!\n",
      "..epoch [54/256], Train Loss: 0.5955, Val Loss: 0.6330\n",
      "..best model saved with val loss = 0.6329731154441833!\n",
      "..epoch [55/256], Train Loss: 0.5952, Val Loss: 0.6289\n",
      "..best model saved with val loss = 0.6288763904571533!\n",
      "..epoch [56/256], Train Loss: 0.5894, Val Loss: 0.6284\n",
      "..best model saved with val loss = 0.6283856725692749!\n",
      "..epoch [57/256], Train Loss: 0.5843, Val Loss: 0.6275\n",
      "..best model saved with val loss = 0.6274559926986695!\n",
      "..epoch [58/256], Train Loss: 0.5788, Val Loss: 0.6207\n",
      "..best model saved with val loss = 0.6207186126708985!\n",
      "..epoch [59/256], Train Loss: 0.5783, Val Loss: 0.6188\n",
      "..best model saved with val loss = 0.6188378632068634!\n",
      "..epoch [60/256], Train Loss: 0.5779, Val Loss: 0.6220\n",
      "..epoch [61/256], Train Loss: 0.5716, Val Loss: 0.6131\n",
      "..best model saved with val loss = 0.613101681470871!\n",
      "..epoch [62/256], Train Loss: 0.5652, Val Loss: 0.6151\n",
      "..epoch [63/256], Train Loss: 0.5638, Val Loss: 0.6108\n",
      "..best model saved with val loss = 0.6108138227462768!\n",
      "..epoch [64/256], Train Loss: 0.5596, Val Loss: 0.6056\n",
      "..best model saved with val loss = 0.6055612313747406!\n",
      "..epoch [65/256], Train Loss: 0.5565, Val Loss: 0.6041\n",
      "..best model saved with val loss = 0.604104106426239!\n",
      "..epoch [66/256], Train Loss: 0.5574, Val Loss: 0.6024\n",
      "..best model saved with val loss = 0.6024494922161102!\n",
      "..epoch [67/256], Train Loss: 0.5512, Val Loss: 0.6030\n",
      "..epoch [68/256], Train Loss: 0.5481, Val Loss: 0.6037\n",
      "..epoch [69/256], Train Loss: 0.5481, Val Loss: 0.5999\n",
      "..best model saved with val loss = 0.5999039030075073!\n",
      "..epoch [70/256], Train Loss: 0.5431, Val Loss: 0.5955\n",
      "..best model saved with val loss = 0.5954738736152649!\n",
      "..epoch [71/256], Train Loss: 0.5421, Val Loss: 0.5951\n",
      "..best model saved with val loss = 0.5950577628612518!\n",
      "..epoch [72/256], Train Loss: 0.5403, Val Loss: 0.5930\n",
      "..best model saved with val loss = 0.592966092824936!\n",
      "..epoch [73/256], Train Loss: 0.5370, Val Loss: 0.5893\n",
      "..best model saved with val loss = 0.5892892158031464!\n",
      "..epoch [74/256], Train Loss: 0.5338, Val Loss: 0.5929\n",
      "..epoch [75/256], Train Loss: 0.5296, Val Loss: 0.5875\n",
      "..best model saved with val loss = 0.5875139820575714!\n",
      "..epoch [76/256], Train Loss: 0.5282, Val Loss: 0.5887\n",
      "..epoch [77/256], Train Loss: 0.5250, Val Loss: 0.5832\n",
      "..best model saved with val loss = 0.5832390117645264!\n",
      "..epoch [78/256], Train Loss: 0.5221, Val Loss: 0.5828\n",
      "..best model saved with val loss = 0.5827692103385925!\n",
      "..epoch [79/256], Train Loss: 0.5194, Val Loss: 0.5826\n",
      "..best model saved with val loss = 0.5826166141033172!\n",
      "..epoch [80/256], Train Loss: 0.5182, Val Loss: 0.5793\n",
      "..best model saved with val loss = 0.5793169450759887!\n",
      "..epoch [81/256], Train Loss: 0.5167, Val Loss: 0.5802\n",
      "..epoch [82/256], Train Loss: 0.5138, Val Loss: 0.5781\n",
      "..best model saved with val loss = 0.5780774533748627!\n",
      "..epoch [83/256], Train Loss: 0.5125, Val Loss: 0.5765\n",
      "..best model saved with val loss = 0.5764784228801727!\n",
      "..epoch [84/256], Train Loss: 0.5078, Val Loss: 0.5764\n",
      "..best model saved with val loss = 0.5764215493202209!\n",
      "..epoch [85/256], Train Loss: 0.5062, Val Loss: 0.5723\n",
      "..best model saved with val loss = 0.5723049283027649!\n",
      "..epoch [86/256], Train Loss: 0.5025, Val Loss: 0.5697\n",
      "..best model saved with val loss = 0.5697007393836975!\n",
      "..epoch [87/256], Train Loss: 0.5044, Val Loss: 0.5736\n",
      "..epoch [88/256], Train Loss: 0.5014, Val Loss: 0.5715\n",
      "..epoch [89/256], Train Loss: 0.5016, Val Loss: 0.5714\n",
      "..epoch [90/256], Train Loss: 0.4990, Val Loss: 0.5691\n",
      "..best model saved with val loss = 0.5690815687179566!\n",
      "..epoch [91/256], Train Loss: 0.4978, Val Loss: 0.5680\n",
      "..best model saved with val loss = 0.5679727625846863!\n",
      "..epoch [92/256], Train Loss: 0.4935, Val Loss: 0.5658\n",
      "..best model saved with val loss = 0.5657893359661103!\n",
      "..epoch [93/256], Train Loss: 0.4924, Val Loss: 0.5662\n",
      "..epoch [94/256], Train Loss: 0.4901, Val Loss: 0.5672\n",
      "..epoch [95/256], Train Loss: 0.4845, Val Loss: 0.5626\n",
      "..best model saved with val loss = 0.5626254272460938!\n",
      "..epoch [96/256], Train Loss: 0.4849, Val Loss: 0.5647\n",
      "..epoch [97/256], Train Loss: 0.4846, Val Loss: 0.5622\n",
      "..best model saved with val loss = 0.5621888720989228!\n",
      "..epoch [98/256], Train Loss: 0.4842, Val Loss: 0.5618\n",
      "..best model saved with val loss = 0.5617556238174438!\n",
      "..epoch [99/256], Train Loss: 0.4776, Val Loss: 0.5599\n",
      "..best model saved with val loss = 0.5598963117599487!\n",
      "..epoch [100/256], Train Loss: 0.4787, Val Loss: 0.5594\n",
      "..best model saved with val loss = 0.5594425535202027!\n",
      "..epoch [101/256], Train Loss: 0.4799, Val Loss: 0.5603\n",
      "..epoch [102/256], Train Loss: 0.4752, Val Loss: 0.5632\n",
      "..epoch [103/256], Train Loss: 0.4716, Val Loss: 0.5573\n",
      "..best model saved with val loss = 0.5572602522373199!\n",
      "..epoch [104/256], Train Loss: 0.4752, Val Loss: 0.5617\n",
      "..epoch [105/256], Train Loss: 0.4720, Val Loss: 0.5573\n",
      "..epoch [106/256], Train Loss: 0.4709, Val Loss: 0.5554\n",
      "..best model saved with val loss = 0.5554040312767029!\n",
      "..epoch [107/256], Train Loss: 0.4656, Val Loss: 0.5551\n",
      "..best model saved with val loss = 0.5551170444488526!\n",
      "..epoch [108/256], Train Loss: 0.4672, Val Loss: 0.5532\n",
      "..best model saved with val loss = 0.5531549489498139!\n",
      "..epoch [109/256], Train Loss: 0.4616, Val Loss: 0.5553\n",
      "..epoch [110/256], Train Loss: 0.4631, Val Loss: 0.5570\n",
      "..epoch [111/256], Train Loss: 0.4595, Val Loss: 0.5556\n",
      "..epoch [112/256], Train Loss: 0.4592, Val Loss: 0.5547\n",
      "..epoch [113/256], Train Loss: 0.4577, Val Loss: 0.5529\n",
      "..best model saved with val loss = 0.5529184615612031!\n",
      "..epoch [114/256], Train Loss: 0.4561, Val Loss: 0.5562\n",
      "..epoch [115/256], Train Loss: 0.4551, Val Loss: 0.5497\n",
      "..best model saved with val loss = 0.5496851992607117!\n",
      "..epoch [116/256], Train Loss: 0.4529, Val Loss: 0.5536\n",
      "..epoch [117/256], Train Loss: 0.4519, Val Loss: 0.5516\n",
      "..epoch [118/256], Train Loss: 0.4520, Val Loss: 0.5511\n",
      "..epoch [119/256], Train Loss: 0.4516, Val Loss: 0.5510\n",
      "..epoch [120/256], Train Loss: 0.4476, Val Loss: 0.5481\n",
      "..best model saved with val loss = 0.5480856907367706!\n",
      "..epoch [121/256], Train Loss: 0.4487, Val Loss: 0.5493\n",
      "..epoch [122/256], Train Loss: 0.4476, Val Loss: 0.5501\n",
      "..epoch [123/256], Train Loss: 0.4447, Val Loss: 0.5525\n",
      "..epoch [124/256], Train Loss: 0.4435, Val Loss: 0.5461\n",
      "..best model saved with val loss = 0.5461098289489746!\n",
      "..epoch [125/256], Train Loss: 0.4430, Val Loss: 0.5533\n",
      "..epoch [126/256], Train Loss: 0.4414, Val Loss: 0.5455\n",
      "..best model saved with val loss = 0.5454900062084198!\n",
      "..epoch [127/256], Train Loss: 0.4439, Val Loss: 0.5493\n",
      "..epoch [128/256], Train Loss: 0.4367, Val Loss: 0.5463\n",
      "..epoch [129/256], Train Loss: 0.4370, Val Loss: 0.5485\n",
      "..epoch [130/256], Train Loss: 0.4369, Val Loss: 0.5469\n",
      "..epoch [131/256], Train Loss: 0.4335, Val Loss: 0.5482\n",
      "..epoch [132/256], Train Loss: 0.4351, Val Loss: 0.5447\n",
      "..best model saved with val loss = 0.5447447645664215!\n",
      "..epoch [133/256], Train Loss: 0.4341, Val Loss: 0.5466\n",
      "..epoch [134/256], Train Loss: 0.4321, Val Loss: 0.5472\n",
      "..epoch [135/256], Train Loss: 0.4333, Val Loss: 0.5466\n",
      "..epoch [136/256], Train Loss: 0.4284, Val Loss: 0.5454\n",
      "..epoch [137/256], Train Loss: 0.4287, Val Loss: 0.5454\n",
      "..epoch [138/256], Train Loss: 0.4277, Val Loss: 0.5415\n",
      "..best model saved with val loss = 0.5415285468101502!\n",
      "..epoch [139/256], Train Loss: 0.4258, Val Loss: 0.5452\n",
      "..epoch [140/256], Train Loss: 0.4251, Val Loss: 0.5437\n",
      "..epoch [141/256], Train Loss: 0.4264, Val Loss: 0.5450\n",
      "..epoch [142/256], Train Loss: 0.4199, Val Loss: 0.5436\n",
      "..epoch [143/256], Train Loss: 0.4228, Val Loss: 0.5405\n",
      "..best model saved with val loss = 0.5404516196250916!\n",
      "..epoch [144/256], Train Loss: 0.4207, Val Loss: 0.5472\n",
      "..epoch [145/256], Train Loss: 0.4207, Val Loss: 0.5406\n",
      "..epoch [146/256], Train Loss: 0.4189, Val Loss: 0.5435\n",
      "..epoch [147/256], Train Loss: 0.4187, Val Loss: 0.5428\n",
      "..epoch [148/256], Train Loss: 0.4156, Val Loss: 0.5437\n",
      "..epoch [149/256], Train Loss: 0.4160, Val Loss: 0.5435\n",
      "..epoch [150/256], Train Loss: 0.4167, Val Loss: 0.5407\n",
      "..epoch [151/256], Train Loss: 0.4120, Val Loss: 0.5451\n",
      "..epoch [152/256], Train Loss: 0.4148, Val Loss: 0.5444\n",
      "..epoch [153/256], Train Loss: 0.4136, Val Loss: 0.5446\n",
      "..epoch [154/256], Train Loss: 0.4102, Val Loss: 0.5449\n",
      "..epoch [155/256], Train Loss: 0.4086, Val Loss: 0.5417\n",
      "..epoch [156/256], Train Loss: 0.4096, Val Loss: 0.5417\n",
      "..epoch [157/256], Train Loss: 0.4110, Val Loss: 0.5409\n",
      "..epoch [158/256], Train Loss: 0.4112, Val Loss: 0.5403\n",
      "..best model saved with val loss = 0.5403386783599854!\n",
      "..epoch [159/256], Train Loss: 0.4090, Val Loss: 0.5400\n",
      "..best model saved with val loss = 0.5400144815444946!\n",
      "..epoch [160/256], Train Loss: 0.4104, Val Loss: 0.5398\n",
      "..best model saved with val loss = 0.5397888171672821!\n",
      "..epoch [161/256], Train Loss: 0.4035, Val Loss: 0.5422\n",
      "..epoch [162/256], Train Loss: 0.4034, Val Loss: 0.5423\n",
      "..epoch [163/256], Train Loss: 0.4046, Val Loss: 0.5409\n",
      "..epoch [164/256], Train Loss: 0.4013, Val Loss: 0.5412\n",
      "..epoch [165/256], Train Loss: 0.4008, Val Loss: 0.5484\n",
      "..epoch [166/256], Train Loss: 0.3982, Val Loss: 0.5404\n",
      "..epoch [167/256], Train Loss: 0.3986, Val Loss: 0.5402\n",
      "..epoch [168/256], Train Loss: 0.3994, Val Loss: 0.5405\n",
      "..epoch [169/256], Train Loss: 0.4018, Val Loss: 0.5425\n",
      "..epoch [170/256], Train Loss: 0.3965, Val Loss: 0.5397\n",
      "..best model saved with val loss = 0.5396764028072357!\n",
      "..epoch [171/256], Train Loss: 0.3976, Val Loss: 0.5375\n",
      "..best model saved with val loss = 0.5374569392204285!\n",
      "..epoch [172/256], Train Loss: 0.3949, Val Loss: 0.5444\n",
      "..epoch [173/256], Train Loss: 0.3944, Val Loss: 0.5392\n",
      "..epoch [174/256], Train Loss: 0.3963, Val Loss: 0.5393\n",
      "..epoch [175/256], Train Loss: 0.3931, Val Loss: 0.5421\n",
      "..epoch [176/256], Train Loss: 0.3924, Val Loss: 0.5437\n",
      "..epoch [177/256], Train Loss: 0.3905, Val Loss: 0.5402\n",
      "..epoch [178/256], Train Loss: 0.3906, Val Loss: 0.5484\n",
      "..epoch [179/256], Train Loss: 0.3918, Val Loss: 0.5421\n",
      "..epoch [180/256], Train Loss: 0.3913, Val Loss: 0.5415\n",
      "..epoch [181/256], Train Loss: 0.3898, Val Loss: 0.5390\n",
      "..epoch [182/256], Train Loss: 0.3851, Val Loss: 0.5411\n",
      "..epoch [183/256], Train Loss: 0.3847, Val Loss: 0.5385\n",
      "..epoch [184/256], Train Loss: 0.3863, Val Loss: 0.5403\n",
      "..epoch [185/256], Train Loss: 0.3865, Val Loss: 0.5462\n",
      "..epoch [186/256], Train Loss: 0.3850, Val Loss: 0.5417\n",
      "..epoch [187/256], Train Loss: 0.3870, Val Loss: 0.5404\n",
      "..epoch [188/256], Train Loss: 0.3829, Val Loss: 0.5414\n",
      "..epoch [189/256], Train Loss: 0.3820, Val Loss: 0.5421\n",
      "..epoch [190/256], Train Loss: 0.3808, Val Loss: 0.5425\n",
      "..epoch [191/256], Train Loss: 0.3839, Val Loss: 0.5406\n",
      "..early stopping triggered.\n",
      "Test Loss: 0.5659\n"
     ]
    }
   ],
   "source": [
    "# training and evaluation\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # working locally on mac, change to 'gpu' if working on a HPC\n",
    "model = Seq2SeqLSTM(vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "num_epochs = 256\n",
    "best_model_path, train_losses, val_losses = train_lstm(model, train_loader, val_loader, num_epochs, criterion, optimizer, device)\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "evaluate_lstm(model, test_loader, device, criterion, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHWCAYAAABACtmGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABxI0lEQVR4nO3dd3hUVf7H8c9MekiBACGhSEc6IhCkgyAEWBBFRUUBFV0VUERWFl2l6IqKrroW1FXhh8gKomBZBIOCVEFFkCYCRkBJQEoSIKTO/f0RMmTSZiZl5iZ5v54nj8y9Z2bOnFwwn5xzvtdiGIYhAAAAAECRrN7uAAAAAACYHcEJAAAAAJwgOAEAAACAEwQnAAAAAHCC4AQAAAAAThCcAAAAAMAJghMAAAAAOEFwAgAAAAAnCE4AAAAA4ATBCQA8bNy4cWrUqJG3u1Eiffv2Vd++fT3+vo0aNdK4ceOctrNYLJo5c2a59weet27dOlksFi1btszbXQFQRRGcAOAii8Xi0te6deu83VUAAOBhvt7uAACYxXvvvefweOHChYqLiytwvFWrVqV6n//85z+y2Wyleg0AAOBZBCcAuOi2225zePztt98qLi6uwPH8UlNTFRwc7PL7+Pn5lah/QGmdP39e1apV83Y3AKBCYqkeALihb9++atu2rX744Qf17t1bwcHBevTRRyVJn3zyiYYOHaq6desqICBATZs21ZNPPqns7GyH18i/x+m3336TxWLR888/r7feektNmzZVQECAunTpou+++85pn06fPq2pU6eqXbt2CgkJUVhYmAYPHqydO3c6tMvdI7J06VL985//VP369RUYGKj+/fvr4MGDBV43ty9BQUGKiYnRhg0bXBqjtm3bql+/fgWO22w21atXTzfccIP92PPPP6/u3burZs2aCgoKUqdOncp8D8uPP/6owYMHKywsTCEhIerfv7++/fZbhzaZmZmaNWuWmjdvrsDAQNWsWVM9e/ZUXFycvU1iYqLuuOMO1a9fXwEBAYqOjta1116r3377zWkfvv76a/Xq1UvVqlVT9erVde2112rfvn3288uWLZPFYtE333xT4LlvvvmmLBaLdu/ebT/2888/64YbblBERIQCAwPVuXNnffrppw7PW7Bggf0177//fkVGRqp+/frF9jM9PV0zZsxQs2bNFBAQoAYNGuiRRx5Renq6QzuLxaKJEyfq/fff1+WXX67AwEB16tRJ69evL/Caroy/JCUlJemhhx5So0aNFBAQoPr162vMmDE6efKkQzubzeb0+j1w4IBGjhypqKgoBQYGqn79+rr55puVnJxc7OcHgOIw4wQAbjp16pQGDx6sm2++Wbfddpvq1KkjKecH1ZCQEE2ZMkUhISH6+uuv9cQTTyglJUVz5851+rqLFy/W2bNn9de//lUWi0XPPfecrr/+ev3666/FzlL9+uuvWrFihW688UY1btxYx48f15tvvqk+ffpo7969qlu3rkP7Z555RlarVVOnTlVycrKee+45jR49Wlu3brW3eeedd/TXv/5V3bt31+TJk/Xrr79q+PDhioiIUIMGDYr9HKNGjdLMmTOVmJioqKgo+/GNGzfq2LFjuvnmm+3HXn75ZQ0fPlyjR49WRkaGPvjgA9144436/PPPNXToUKdj5syePXvUq1cvhYWF6ZFHHpGfn5/efPNN9e3bV9988426du0qSZo5c6bmzJmj8ePHKyYmRikpKfr++++1fft2XXPNNZKkkSNHas+ePZo0aZIaNWqkEydOKC4uTkeOHCm22MeaNWs0ePBgNWnSRDNnztSFCxf0yiuvqEePHtq+fbsaNWqkoUOHKiQkREuXLlWfPn0cnr9kyRK1adNGbdu2tX+mHj16qF69evr73/+uatWqaenSpRoxYoQ++ugjXXfddQ7Pv//++1W7dm098cQTOn/+fJH9tNlsGj58uDZu3Kh77rlHrVq10q5du/Tiiy/ql19+0YoVKxzaf/PNN1qyZIkeeOABBQQE6PXXX1dsbKy2bdvm0FdXxv/cuXPq1auX9u3bpzvvvFNXXnmlTp48qU8//VS///67atWqZX9fZ9dvRkaGBg0apPT0dE2aNElRUVH6448/9PnnnyspKUnh4eFFjgEAFMsAABRqwoQJRv5/Jvv06WNIMt54440C7VNTUwsc++tf/2oEBwcbaWlp9mNjx441GjZsaH8cHx9vSDJq1qxpnD592n78k08+MSQZn332WbH9TEtLM7Kzsx2OxcfHGwEBAcbs2bPtx9auXWtIMlq1amWkp6fbj7/88suGJGPXrl2GYRhGRkaGERkZaVxxxRUO7d566y1DktGnT59i+7N//35DkvHKK684HL///vuNkJAQh3HKP2YZGRlG27ZtjauvvtrheMOGDY2xY8cW+76GYRiSjBkzZtgfjxgxwvD39zcOHTpkP3bs2DEjNDTU6N27t/1Yhw4djKFDhxb5umfOnDEkGXPnznXah/yuuOIKIzIy0jh16pT92M6dOw2r1WqMGTPGfuyWW24xIiMjjaysLPuxhIQEw2q1Onwf+/fvb7Rr187hmrLZbEb37t2N5s2b24/Nnz/fkGT07NnT4TWL8t577xlWq9XYsGGDw/E33njDkGRs2rTJfkySIcn4/vvv7ccOHz5sBAYGGtddd539mKvj/8QTTxiSjI8//rhAv2w2m2EYrl+/P/74oyHJ+PDDD51+ZgBwB0v1AMBNAQEBuuOOOwocDwoKsv/57NmzOnnypHr16qXU1FT9/PPPTl931KhRqlGjhv1xr169JOXMKDnrj9Wa8895dna2Tp06pZCQEF1++eXavn17gfZ33HGH/P39i3yf77//XidOnNC9997r0G7cuHEu/ba+RYsWuuKKK7RkyRL7sezsbC1btkzDhg1zGKe8fz5z5oySk5PVq1evQvvtruzsbH355ZcaMWKEmjRpYj8eHR2tW2+9VRs3blRKSookqXr16tqzZ48OHDhQ6GsFBQXJ399f69at05kzZ1zuQ0JCgnbs2KFx48YpIiLCfrx9+/a65pprtHLlSvuxUaNG6cSJEw5VG5ctWyabzaZRo0ZJylmW+fXXX+umm26yX2MnT57UqVOnNGjQIB04cEB//PGHQx/uvvtu+fj4OO3rhx9+qFatWqlly5b21z158qSuvvpqSdLatWsd2nfr1k2dOnWyP77ssst07bXXavXq1crOznZr/D/66CN16NChwGyZlLMsMC9n12/uNbp69WqlpqY6/dwA4CqCEwC4qV69eg4/uOXas2ePrrvuOoWHhyssLEy1a9e2F5ZwZW/FZZdd5vA4N0Q5+0HdZrPpxRdfVPPmzRUQEKBatWqpdu3a+umnnwp9X2fvc/jwYUlS8+bNHdr5+fk5/ABcnFGjRmnTpk32H+LXrVunEydO2ANArs8//1xXXXWVAgMDFRERodq1a2vevHllshflzz//VGpqqi6//PIC51q1aiWbzaajR49KkmbPnq2kpCS1aNFC7dq109/+9jf99NNP9vYBAQF69tln9cUXX6hOnTrq3bu3nnvuOSUmJhbbh9yxLKoPJ0+etC+fi42NVXh4uEPgXLJkia644gq1aNFCknTw4EEZhqHHH39ctWvXdviaMWOGJOnEiRMO79O4cWOnYyXl7Avas2dPgdfNfe/8r5v/+pByQnNqaqr+/PNPt8b/0KFD9uV9zji7fhs3bqwpU6bo7bffVq1atTRo0CC99tpr7G8CUGoEJwBwU95ZklxJSUnq06ePdu7cqdmzZ+uzzz5TXFycnn32WUlyqfx4UbMChmEU+7ynn35aU6ZMUe/evbVo0SKtXr1acXFxatOmTaHvW9L3cceoUaNkGIY+/PBDSdLSpUsVHh6u2NhYe5sNGzZo+PDhCgwM1Ouvv66VK1cqLi5Ot956a5n2xRW9e/fWoUOH9O6776pt27Z6++23deWVV+rtt9+2t5k8ebJ++eUXzZkzR4GBgXr88cfVqlUr/fjjj2XSh4CAAI0YMULLly9XVlaW/vjjD23atMkhbOZ+P6dOnaq4uLhCv5o1a+bwuoVdr4Wx2Wxq165dka97//33l8nnLC1Xrt8XXnhBP/30kx599FFduHBBDzzwgNq0aaPff//dU90EUAlRHAIAysC6det06tQpffzxx+rdu7f9eHx8fLm/97Jly9SvXz+98847DseTkpIcNtW7qmHDhpJyZiByl2lJOZXn4uPj1aFDB6ev0bhxY8XExGjJkiWaOHGiPv74Y40YMUIBAQH2Nh999JECAwO1evVqh+Pz5893u8+FqV27toKDg7V///4C537++WdZrVaHQhcRERG64447dMcdd+jcuXPq3bu3Zs6cqfHjx9vbNG3aVA8//LAefvhhHThwQFdccYVeeOEFLVq0qNA+5I5lUX2oVauWQ3nwUaNG6f/+7//01Vdfad++fTIMwyE45c74+fn5acCAAW6OSPGaNm2qnTt3qn///gWWxxWmsGWNv/zyi4KDg1W7dm1Jcnn8mzZt6lA1sCy0a9dO7dq10z/+8Q9t3rxZPXr00BtvvKGnnnqqTN8HQNXBjBMAlIHc34Ln/a13RkaGXn/9dY+8d/4Zmg8//LDAXhdXde7cWbVr19Ybb7yhjIwM+/EFCxYoKSnJ5dcZNWqUvv32W7377rs6efJkgWV6Pj4+slgsDuXaf/vttwLV20rKx8dHAwcO1CeffOJQMvz48eNavHixevbsqbCwMEk5lRLzCgkJUbNmzexluFNTU5WWlubQpmnTpgoNDS1Qqjuv6OhoXXHFFfq///s/h7HbvXu3vvzySw0ZMsSh/YABAxQREaElS5ZoyZIliomJcVhqFxkZqb59++rNN99UQkJCgff7888/ix+UYtx00036448/9J///KfAuQsXLhSoyLdlyxaHvWhHjx7VJ598ooEDB8rHx8et8R85cqR27typ5cuXF3hvd2cfU1JSlJWV5XCsXbt2slqtxX6vAMAZZpwAoAx0795dNWrU0NixY/XAAw/IYrHovffe88iSs7/85S+aPXu27rjjDnXv3l27du3S+++/7/J+pPz8/Pz01FNP6a9//auuvvpqjRo1SvHx8Zo/f75br3nTTTdp6tSpmjp1qiIiIgrMkAwdOlT/+te/FBsbq1tvvVUnTpzQa6+9pmbNmjnsLyqNp556SnFxcerZs6fuv/9++fr66s0331R6erqee+45e7vWrVurb9++6tSpkyIiIvT9999r2bJlmjhxoqScmZT+/fvrpptuUuvWreXr66vly5fr+PHjDuXVCzN37lwNHjxY3bp101133WUvRx4eHq6ZM2c6tPXz89P111+vDz74QOfPn9fzzz9f4PVee+019ezZU+3atdPdd9+tJk2a6Pjx49qyZYt+//33AvfvctXtt9+upUuX6t5779XatWvVo0cPZWdn6+eff9bSpUu1evVqde7c2d6+bdu2GjRokEM5ckmaNWuWvY2r4/+3v/1Ny5Yt04033qg777xTnTp10unTp/Xpp5/qjTfecGmWM9fXX3+tiRMn6sYbb1SLFi2UlZWl9957Tz4+Pho5cmSJxgYAJFGOHACKUlQ58jZt2hTaftOmTcZVV11lBAUFGXXr1jUeeeQRY/Xq1YYkY+3atfZ2RZUjL6zUtfKV1y5MWlqa8fDDDxvR0dFGUFCQ0aNHD2PLli1Gnz59HEqH55Zzzl+mOff958+f73D89ddfNxo3bmwEBAQYnTt3NtavX1/gNZ3p0aOHIckYP358oeffeecdo3nz5kZAQIDRsmVLY/78+caMGTMKjHtJy5EbhmFs377dGDRokBESEmIEBwcb/fr1MzZv3uzQ5qmnnjJiYmKM6tWrG0FBQUbLli2Nf/7zn0ZGRoZhGIZx8uRJY8KECUbLli2NatWqGeHh4UbXrl2NpUuXujQOa9asMXr06GEEBQUZYWFhxrBhw4y9e/cW2jYuLs6QZFgsFuPo0aOFtjl06JAxZswYIyoqyvDz8zPq1atn/OUvfzGWLVtmb5Nbjvy7775zqY+GkVMO/tlnnzXatGljBAQEGDVq1DA6depkzJo1y0hOTra3k2RMmDDBWLRokf3717FjR4frPJcr428YhnHq1Clj4sSJRr169Qx/f3+jfv36xtixY42TJ08ahuH69fvrr78ad955p9G0aVMjMDDQiIiIMPr162esWbPG5XEAgMJYDMPDO3ABAECFZrFYNGHCBL366qve7goAeAx7nAAAAADACYITAAAAADhBcAIAAAAAJ6iqBwAA3ML2aABVETNOAAAAAOAEwQkAAAAAnKhyS/VsNpuOHTum0NBQWSwWb3cHAAAAgJcYhqGzZ8+qbt26slqLn1OqcsHp2LFjatCggbe7AQAAAMAkjh49qvr16xfbpsoFp9DQUEk5gxMWFubl3kiZmZn68ssvNXDgQPn5+Xm7O5Ue4+15jLnnMeaexXh7HmPueYy5ZzHenpOSkqIGDRrYM0Jxqlxwyl2eFxYWZprgFBwcrLCwMP5ieADj7XmMuecx5p7FeHseY+55jLlnMd6e58oWHopDAAAAAIATBCcAAAAAcILgBAAAAABOVLk9TgAAADAfwzCUlZWl7Oxsb3fF6zIzM+Xr66u0tDTGowz4+fnJx8en1K9DcAIAAIBXZWRkKCEhQampqd7uiikYhqGoqCgdPXqU+46WAYvFovr16yskJKRUr0NwAgAAgNfYbDbFx8fLx8dHdevWlb+/f5UPCzabTefOnVNISIjTm7KieIZh6M8//9Tvv/+u5s2bl2rmieAEAAAAr8nIyJDNZlODBg0UHBzs7e6Ygs1mU0ZGhgIDAwlOZaB27dr67bfflJmZWargxHcCAAAAXkdAQHkpqxlMrlAAAAAAcILg5EXZNkNb40/rh5MWbY0/rWyb4e0uAQAAACgEwclLVu1OUM9nv9Zt736vhQd8dNu736vns19r1e4Eb3cNAACgwsm2Gdpy6JQ+2fGHthw6VSF/Id2oUSO99NJLLrdft26dLBaLkpKSyq1PuITiEF6waneC7lu0Xfn/Oicmp+m+Rds177YrFds22it9AwAAqGhW7U7QrM/2KiE5zX4sOjxQM4a1LpefqZztmZkxY4Zmzpzp9ut+9913qlatmsvtu3fvroSEBIWHh7v9Xu5Yt26d+vXrpzNnzqh69erl+l5mxoyTh2XbDM36bG+B0CTJfmzWZ3sr5G9JAAAAPC33F9J5Q5N06RfS5bGaJyEhwf710ksvKSwszOHY1KlT7W1zb+zritq1a7tVWdDf319RUVFVvny7pxCcPGxb/OkCf7HzMiQlJKdpW/xpz3UKAADAJAzDUGpGlktfZ9MyNePTPcX+Qnrmp3t1Ni3TpdczDNd+cR0VFWX/Cg8Pl8VisT/++eefFRoaqi+++EKdOnVSQECANm7cqEOHDunaa69VnTp1FBISoi5dumjNmjUOr5t/qZ6Pj4/efvttXXfddQoODlbz5s316aef2s/nX6q3YMECVa9eXatXr1arVq0UEhKi2NhYJSRcCo9ZWVl64IEHVL16ddWsWVPTpk3T2LFjNWLECJc+e2HOnDmjMWPGqEaNGgoODtbgwYN14MAB+/nDhw9r2LBhqlGjhqpVq6Y2bdpo5cqV9ueOHj1atWvXVlBQkJo3b6758+eXuC/liaV6HnbibNGhqSTtAAAAKpMLmdlq/cTqMnktQ1JiSprazfzSpfZ7Zw9SsH/Z/Hj897//Xc8//7yaNGmiGjVq6OjRoxoyZIj++c9/KiAgQAsXLtSwYcO0f/9+XXbZZUW+zqxZs/Tcc89p7ty5euWVVzR69GgdPnxYERERhbZPTU3V888/r/fee09Wq1W33Xabpk6dqvfff1+S9Oyzz+r999/X/Pnz1apVK7388stasWKF+vXrV+LPOm7cOB04cECffvqpwsLCNG3aNA0ZMkR79+6Vn5+fJkyYoIyMDK1fv17VqlXT3r17FRISIkl6/PHHtXfvXn3xxReqVauWDh48qAsXLpS4L+WJ4ORhkaGBZdoOAAAA5jN79mxdc8019scRERHq0KGD/fGTTz6p5cuX69NPP9XEiROLfJ1x48bplltukSQ9/fTT+ve//61t27YpNja20PaZmZl644031LRpU0nSxIkTNXv2bPv5V155RdOnT9d1110nSXr11Vftsz8lkRuYNm3apO7du0uS3n//fTVo0EArVqzQjTfeqCNHjmjkyJFq166dJKlJkyb25x85ckQdO3ZU586dJeXMupkVwcnDYhpHKDo8UInJaYVOK1skRYUHKqZx4b9FAAAAqMyC/Hy0d/Ygl9puiz+tcfO/c9puwR1dXPrZKsjPx6X3dUVuEMh17tw5zZw5U//73/+UkJCgrKwsXbhwQUeOHCn2ddq3b2//c7Vq1RQWFqYTJ04U2T44ONgemiQpOjra3j45OVnHjx9XTEyM/byPj486deokm83m1ufLtW/fPvn6+qpr1672YzVr1tTll1+uffv2SZIeeOAB3Xffffryyy81YMAAjRw50v657rvvPo0cOVLbt2/XwIEDNWLECHsAMxv2OHmYj9WiGcNaF3oud1vfjGGt5WNlkx8AAKh6LBaLgv19Xfrq1by2osMDVdRPTRblVNfr1by2S69XlkUW8lfHmzp1qpYvX66nn35aGzZs0I4dO9SuXTtlZGQU+zp+fn6On8liKTbkFNbe1b1b5WX8+PH69ddfdfvtt2vXrl3q3LmzXnnlFUnS4MGDdfjwYT300EM6duyY+vfv71Bcw0wITl4Q2zZa8267UqGBjhN+UeGBlCIHAABwUd5fSOePPGb7hfSmTZs0btw4XXfddWrXrp2ioqL022+/ebQP4eHhqlOnjr777tIsXXZ2trZv317i12zVqpWysrK0detW+7FTp05p//79at360mRBgwYNdO+99+rjjz/Www8/rP/85z/2c7Vr19bYsWO1aNEivfTSS3rrrbdK3J/yxFI9L4ltG63jZ9M145M9uqyaTU+PilG3ZpGm+IsNAABQUeT+Qjr/fZyiyvE+TiXRvHlzffzxxxo2bJgsFosef/zxEi+PK41JkyZpzpw5atasmVq2bKlXXnlFZ86ccWm2bdeuXQoNDbU/tlgs6tChg6699lrdfffdevPNNxUaGqq///3vqlevnq699lpJ0uTJkzV48GC1aNFCZ86c0dq1a9WqVStJ0hNPPKFOnTqpTZs2Sk9P1+eff24/ZzYEJy8K9M2Z8Av1l7o2jiA0AQAAlEBs22hd0zpK2+JP68TZNEWG5uwXN9PPVv/617905513qnv37qpVq5amTZumlJQUj/dj2rRpSkxM1JgxY+Tj46N77rlHgwYNko+P8/1dvXv3dnjs4+OjrKwszZ8/Xw8++KD+8pe/KCMjQ71799bKlSvtywazs7M1YcIE/f777woLC1NsbKxefPFFSTn3opo+fbp+++03BQUFqVevXvrggw/K/oOXAYvh7UWPHpaSkqLw8HAlJycrLCzMq335ePvvmrJ0p1qG2/TZ1NgCa1JR9jIzM7Vy5UoNGTKE8fYQxtzzGHPPYrw9jzH3vPIc87S0NMXHx6tx48YKDKSqsCTZbDalpKQoLCxMVmv57qyx2Wxq1aqVbrrpJj355JPl+l7eUtw15k42YMbJi/x8cv4iZFWp6AoAAABvOXz4sL788kv16dNH6enpevXVVxUfH69bb73V210zPYpDeJH/xaV62YZ5ppEBAABQeVmtVi1YsEBdunRRjx49tGvXLq1Zs8a0+4rMhBknL/LPnXHy/L5AAAAAVEENGjTQpk2bvN2NCokZJy9iqR4AAABQMRCcvMjPJ2eJXjYzTgAAAICpEZy86NIeJy93BAAAAECxvBqc5syZoy5duig0NFSRkZEaMWKE9u/fX+xzFixYIIvF4vBVUUtXslQPAAAAqBi8Gpy++eYbTZgwQd9++63i4uKUmZmpgQMH6vz588U+LywsTAkJCfavw4cPe6jHZSt3xoniEAAAAIC5ebWq3qpVqxweL1iwQJGRkfrhhx8K3Jk4L4vFoqioqPLuXrnLnXFiqR4AAABgbqYqR56cnCxJioiIKLbduXPn1LBhQ9lsNl155ZV6+umn1aZNm0LbpqenKz093f44JSVFUs4dsDMzM8uo5yVjNbIl5RSH8HZfqorccWa8PYcx9zzG3LMYb89jzD2vPMc8MzNThmHIZrPJZivFMhxbtnRki3QuUQqJki7rJll9yq6j5eDqq69Whw4d9OKLL0qSmjRpogcffFAPPPCAJNnHJS8fHx999NFHGjFiRKneu6xepyKw2WwyDEOZmZny8XG8Jty5pi2GYZhivsNms2n48OFKSkrSxo0bi2y3ZcsWHThwQO3bt1dycrKef/55rV+/Xnv27FH9+vULtJ85c6ZmzZpV4PjixYsVHBxcpp/BXSkZ0uM/+MoiQy91y/ZqXwAAALzB19dXUVFRatCggfz9/Uv0Gn4Hv1DQulmynkuwH7OFROtC3xnKbDa4rLpqd/PNNysrK0vLli0rcG7z5s0aOnSoNmzYoLZt2xb7On/5y1/Url07zZkzR5J08uRJBQcHF/szao0aNbRo0SINHTrUpb4+88wz+t///qcNGzY4HD9+/LiqV6+ugIAAl16nJBYvXqzp06d7fVtNRkaGjh49qsTERGVlZTmcS01N1a233qrk5GSFhYUV+zqmmXGaMGGCdu/eXWxokqRu3bqpW7du9sfdu3dXq1at9Oabb+rJJ58s0H769OmaMmWK/XFKSooaNGiggQMHOh2c8paUmqnHf1grQxb1699fQeV44SJHZmam4uLidM0118jPz8/b3akSGHPPY8w9i/H2PMbc88pzzNPS0nT06FGFhISUrODXvs9k+fw+SY5zAZZziQr+/D4ZN/6f1GpY2XT2onvuuUc33nijUlJSCvzi/sMPP1Tnzp3VvXt3p6/j6+srf39/+8+kuf81DENnz55VaGioLBZLgecFBQW5/HNsQECAfHx8CrT3xM/BgYGBslgsXv+ZOy0tTUFBQerdu3eBayx3NZorTBGcJk6cqM8//1zr168vdNaoOH5+furYsaMOHjxY6PmAgIBCk7Sfn5/X/7ENDszzF8Hi6/X+VCVm+P5XNYy55zHmnsV4ex5j7nnlMebZ2dmyWCyyWq2yWq2SYUiZqa492ZYtrZqm/KFJkiwyJFlkWf13qWk/15bt+QVLhQSV/IYPH67atWtr4cKF+sc//mE/fu7cOS1btkxz587VmTNnNHHiRK1fv15nzpxR06ZN9eijj+qWW25x7OfFzy5JjRo10uTJk+1L9Q4ePKi7775b27ZtU5MmTfTyyy9L0qWxkjRt2jQtX75cv//+u6KiojR69Gg98cQT8vPz04IFCzR79mxJsi9Rmz9/vsaNGyeLxaLly5fbl+rt2rVLDz74oLZs2aLg4GCNHDlS//rXvxQSEiJJGjdunJKSktSzZ0+98MILysjI0M0336yXXnqpyGsit4+5/83vyJEjmjRpkr766itZrVbFxsbqlVdeUZ06dSRJO3fu1OTJk/X999/LYrGoefPmevPNN9W5c2cdPnxYEydO1MaNG5WRkaFGjRpp7ty5GjJkSKH9sFgshV6/7lzPXg1OhmFo0qRJWr58udatW6fGjRu7/RrZ2dnatWtXoYNkdrlV9SQpk7vgAgAA5ISmp+uW0YsZUsox6ZkGrjV/9JjkX81pM19fX40ZM0YLFizQY489Zp8V+vDDD5Wdna1bbrlF586dU6dOnTRt2jSFhYXpf//7n26//XY1bdpUMTExTt/DZrPphhtuUJ06dbR161YlJydr8uTJBdqFhoZqwYIFqlu3rnbt2qW7775boaGheuSRRzRq1Cjt3r1bq1at0po1ayRJ4eHhBV7j/PnzGjRokLp166bvvvtOJ06c0Pjx4zVx4kQtWLDA3m7t2rWKjo7W2rVrdfDgQY0aNUpXXHGF7r77bqefp7DPd+211yokJETffPONsrKyNGHCBI0aNUrr1q2TJI0ePVodO3bUvHnz5OPjox07dtiDzoQJE5SRkaH169erWrVq2rt3rz3klRevBqcJEyZo8eLF+uSTTxQaGqrExERJOd/QoKAgSdKYMWNUr149+9rP2bNn66qrrlKzZs2UlJSkuXPn6vDhwxo/frzXPkdJ+Vov/UaD4AQAAFBx3HnnnZo7d66++eYb9e3bV1LObM7IkSMVHh6u8PBwTZ061d5+0qRJWr16tZYuXepScFq3bp1+/vlnrV69WnXr5gTJp59+WoMHO+7Zyjvj1ahRI02dOlUffPCBHnnkEQUFBSkkJMS+j6woixcvVlpamhYuXKhq1XKC46uvvqphw4bp2Weftc8A1ahRQ6+++qp8fHzUsmVLDR06VF999VWJgtNXX32lXbt2KT4+Xg0a5ATbhQsXqk2bNvruu+/UpUsXHTlyRH/729/UsmVLSVLz5s3tzz9y5IhGjhypdu3aScoprFHevBqc5s2bJ0n2iy1X7hSilDMoeaf3zpw5o7vvvluJiYmqUaOGOnXqpM2bN6t169ae6naZsVgs8vOxKDPbUAY1yQEAAHKWyz16zLW2hzdL79/gvN3oZVJD53uO5Od64bCWLVuqe/fuevfdd9W3b18dPHhQGzZssC+Ny87O1tNPP62lS5fqjz/+UEZGhtLT010uTvbLL7+oQYMG9tAkyWGff64lS5bo3//+tw4dOqRz584pKyvL7T1F+/btU4cOHeyhSZJ69Oghm82m/fv324NTmzZtHKrSRUdHa9euXW69V973bNCggT00SVLr1q1VvXp17du3T126dNGUKVM0fvx4vffeexowYIBuvPFGNW3aVJL0wAMP6L777tOXX36pAQMGaOTIkWrfvn2J+uIqr94A1zCMQr9yQ5OUk7bzThG++OKLOnz4sNLT05WYmKj//e9/6tixo+c7X0b8L97LKYMZJwAAgJw9Rv7VXPtqerUUVldSUfuSLFJYvZx2rryeC/ub8rrrrrv00Ucf6ezZs5o/f76aNm2qPn36SJLmzp2rl19+WdOmTdPatWu1Y8cODRo0SBkZGaUbnzy2bNmi0aNHa8iQIfr888/1448/6rHHHivT98gr/34gi8VSuhLyTsycOVN79uzR0KFD9fXXX6t169Zavny5JGn8+PH69ddfdfvtt2vXrl3q3LmzXnnllXLri+Tl4IRL+5wyswhOAAAAbrH6SLHPXnyQP/RcfBz7TLndz+mmm26S1WrV4sWLtXDhQt155532/U6bNm3Stddeq9tuu00dOnRQkyZN9Msvv7j82i1atNDRo0eVkHCpxPq3337r0Gbz5s1q2LChHnvsMXXu3FnNmzcvUPrb399f2dnF3/amVatW2rlzp86fP28/tmnTJlmtVl1++eUu99kdrVq10tGjR3X06FH7sb179yopKclhJVmLFi300EMP6csvv9T111+v+fPn2881aNBA9957rz7++GM9/PDD+s9//lMufc1FcPIyv4szTpks1QMAAHBf6+HSTQulsGjH42F1c463Hl5ubx0SEqJRo0Zp+vTpSkhIcFg11bx5c8XFxWnz5s3at2+f/vrXv+r48eMuv3bfvn3VokULjR07Vjt37tSGDRv02GOPObRp3ry5jhw5og8++ECHDh3Sv//9b/uMTK5GjRopPj5eO3bs0MmTJ5Wenl7gvUaPHq3AwECNHTtWu3fv1tq1azVp0iTdfvvt9mV6JZWdna0dO3Y4fO3bt08DBgxQu3btNHr0aG3fvl3btm3TmDFj1KdPH3Xu3FkXLlzQxIkTtW7dOh0+fFibNm3Sd999p1atWkmSJk+erNWrVys+Pl7bt2/X2rVr7efKC8HJy/x8cn4rwVI9AACAEmo9XJq8Wxr7uTTynZz/Tt5VrqEp11133aUzZ85o0KBBDvuR/vGPf+jKK6/UoEGD1LdvX0VFRdlLf7vCarXqo48+0oULFxQTE6Px48frn//8p0Ob4cOH66GHHtLEiRN1xRVXaPPmzXr88ccd2owcOVKxsbHq16+fateurf/+978F3is4OFirV6/W6dOn1aVLF91www3q37+/Xn31VfcGoxDnzp1Tx44dHb6GDRsmi8WiTz75RDVq1FDv3r01YMAANWnSREuWLJGUUz791KlTGjNmjFq0aKGbbrpJgwcP1qxZsyTlBLIJEyaoVatWio2NVYsWLfT666+Xur/FsRiGUaWmOlJSUhQeHu7S3YE9oe/ctfrtVKr+O76LujWL9HZ3Kr3MzEytXLlSQ4YM4d4fHsKYex5j7lmMt+cx5p5XnmOelpam+Ph4NW7cuGQ3wK2EbDabUlJSFBYWVuQ9kOC64q4xd7IB3wkv87cv1WPGCQAAADArgpOX+fnmLNVjjxMAAABgXgQnL8stDpFBVT0AAADAtAhOXubHUj0AAADA9AhOXnapqh5L9QAAQNVVxeqVwYPK6toiOHkZxSEAAEBVllulLzU11cs9QWWVkZEhKafEeWn4lkVnUHLscQIAAFWZj4+PqlevrhMnTkjKuaeQxWLxcq+8y2azKSMjQ2lpaZQjLyWbzaY///xTwcHB8vUtXfQhOHkZM04AAKCqi4qKkiR7eKrqDMPQhQsXFBQUVOVDZFmwWq267LLLSj2WBCcvy93jRDlyAABQVVksFkVHRysyMlKZmZne7o7XZWZmav369erduzc3eS4D/v7+ZTJzR3DyMn9fZpwAAACknGV7pd2HUhn4+PgoKytLgYGBBCcTYdGkl7HHCQAAADA/gpOXsVQPAAAAMD+Ck5dxA1wAAADA/AhOXsYeJwAAAMD8CE5eZt/jRHACAAAATIvg5GW5e5wy2OMEAAAAmBbBycvse5yoqgcAAACYFsHJy9jjBAAAAJgfwcnL/O1L9QhOAAAAgFkRnLzsUjly9jgBAAAAZkVw8jLu4wQAAACYH8HJy/yZcQIAAABMj+DkZX6+F/c4UVUPAAAAMC2Ck5exVA8AAAAwP4KTl+XeAJfgBAAAAJgXwcnLcvc4ZbDHCQAAADAtgpOX2ZfqsccJAAAAMC2Ck5ddmnEiOAEAAABmRXDystyqepQjBwAAAMyL4ORl/lTVAwAAAEyP4ORlfnmW6hkGs04AAACAGRGcvCw3OBmGlG0jOAEAAABmRHDystz7OEnscwIAAADMiuDkZf6+l74FVNYDAAAAzIng5GW+1kszThncywkAAAAwJYKTl1ksFvlYcpboUVkPAAAAMCeCkwlcvJUTwQkAAAAwKYKTCeRucyI4AQAAAOZEcDKB3MJ66exxAgAAAEyJ4GQCl2acKEcOAAAAmBHByQR82OMEAAAAmBrByQTsxSFYqgcAAACYEsHJBHwufhfSmXECAAAATIngZALMOAEAAADmRnAygUt7nCgOAQAAAJgRwckEfKw5gYniEAAAAIA5EZxMIHepXgZL9QAAAABTIjiZQO59nDKYcQIAAABMieBkAtzHCQAAADA3gpMJ+BKcAAAAAFMjOJmAfakee5wAAAAAUyI4mUDuUr0MypEDAAAApkRwMgGfi98FluoBAAAA5kRwMgH7HieW6gEAAACmRHAyAft9nJhxAgAAAEyJ4GQCPtacvU0s1QMAAADMieBkAvYZpyyKQwAAAABmRHAyAYpDAAAAAOZGcDKBSzNOBCcAAADAjAhOJpB7HydmnAAAAABzIjiZgO/F7wJV9QAAAABzIjiZADNOAAAAgLkRnEzAPuPEHicAAADAlAhOJnBpxoly5AAAAIAZeTU4zZkzR126dFFoaKgiIyM1YsQI7d+/3+nzPvzwQ7Vs2VKBgYFq166dVq5c6YHelh9fluoBAAAApubV4PTNN99owoQJ+vbbbxUXF6fMzEwNHDhQ58+fL/I5mzdv1i233KK77rpLP/74o0aMGKERI0Zo9+7dHux52fKx5sw0URwCAAAAMCdfb775qlWrHB4vWLBAkZGR+uGHH9S7d+9Cn/Pyyy8rNjZWf/vb3yRJTz75pOLi4vTqq6/qjTfeKNA+PT1d6enp9scpKSmSpMzMTGVmZpbVRymxzMzMS/dxysw2RZ8qs9zxZZw9hzH3PMbcsxhvz2PMPY8x9yzG23PcGWOvBqf8kpOTJUkRERFFttmyZYumTJnicGzQoEFasWJFoe3nzJmjWbNmFTj+5ZdfKjg4uOSdLUO5xSHOnr9Q4ZcdVhRxcXHe7kKVw5h7HmPuWYy35zHmnseYexbjXf5SU1Ndbmua4GSz2TR58mT16NFDbdu2LbJdYmKi6tSp43CsTp06SkxMLLT99OnTHYJWSkqKGjRooIEDByosLKxsOl8KmZmZWvhJzl8Kq5+/hgzp5+UeVW6ZmZmKi4vTNddcIz8/P293p0pgzD2PMfcsxtvzGHPPY8w9i/H2nNzVaK4wTXCaMGGCdu/erY0bN5bp6wYEBCggIKDAcT8/P9NciLlV9bKyDdP0qbIz0/e/qmDMPY8x9yzG2/MYc89jzD2L8S5/7oyvKYLTxIkT9fnnn2v9+vWqX79+sW2joqJ0/Phxh2PHjx9XVFRUeXaxXNnv40RxCAAAAMCUvFpVzzAMTZw4UcuXL9fXX3+txo0bO31Ot27d9NVXXzkci4uLU7du3cqrm+XOXhwi2ybD4F5OAAAAgNl4dcZpwoQJWrx4sT755BOFhoba9ymFh4crKChIkjRmzBjVq1dPc+bMkSQ9+OCD6tOnj1544QUNHTpUH3zwgb7//nu99dZbXvscpZW7VM8wpGybId/cAwAAAABMwaszTvPmzVNycrL69u2r6Oho+9eSJUvsbY4cOaKEhAT74+7du2vx4sV666231KFDBy1btkwrVqwotqCE2fnm+S5kZjPjBAAAAJiNV2ecXFmWtm7dugLHbrzxRt14443l0CPv8M0zwZSRbVOQfLzXGQAAAAAFeHXGCTmseYNTFgUiAAAAALMhOJmAxSL5XdzXlEllPQAAAMB0CE4m4e+T860gOAEAAADmQ3AyCX9fghMAAABgVgQnk/C7OOOUzh4nAAAAwHQITiZxaY8T5cgBAAAAsyE4mYQfe5wAAAAA0yI4mYS9OARL9QAAAADTITiZhN/Fu+CmM+MEAAAAmA7ByST8mHECAAAATIvgZBKX9jhRHAIAAAAwG4KTSVyqqseMEwAAAGA2BCeTyC0OkcFSPQAAAMB0CE4mkbtUL4MZJwAAAMB0CE4m4c99nAAAAADTIjiZBHucAAAAAPMiOJmEvy97nAAAAACz8vV2B6o0W7Yshzeq3uktauGTJKsilEE5cgAAAMB0CE7esvdTadU0+aYcU2dJnSUNCYjQ1j8fkdTCy50DAAAAkBdL9bxh76fS0jFSyjGHw1E6rRG//D3nPAAAAADTIDh5mi1bWjVNUsEleVbLxT+s+ntOOwAAAACmQHDytMObC8w05WWRpJQ/ctoBAAAAMAWCk6edO1627QAAAACUO4KTp4XUKdt2AAAAAModwcnTGnaXwurq4qK8AgxJCquX0w4AAACAKRCcPM3qI8U+W+gpW269iNhnctoBAAAAMAWCkze0Hi7dtFCqVtvhcKJq6t81n8g5DwAAAMA0CE7e0nq4dOeXkiSbfLSx27vqmf6ytgSwRA8AAAAwG4KTN4XkzDhZla1ztTrIJqsysmxe7hQAAACA/AhO3uQfIsPqK0kKtiVLkjKzC94YFwAAAIB3EZy8yWKRgiIkSUFZZyVJmdnMOAEAAABmQ3DytqDqOf/JSpIkZRCcAAAAANMhOHmZcXHGKTArRZLY4wQAAACYEMHJ2wKrS5ICMnL3OBGcAAAAALMhOHnbxRkn/0yKQwAAAABmRXDyMuPiHif/zCRJUiZL9QAAAADTITh528UZJ7/0nBmndJbqAQAAAKZDcPK2izNOPulJknL2OBkGy/UAAAAAMyE4eVluVT3fi8HJMKRsG8EJAAAAMBOCk7ddnHGypp2xH6JABAAAAGAuBCcvy51xsl6ccZK4lxMAAABgNgQnbwuqkfPfC2ck5cw0ZVAgAgAAADAVgpO3XQxOFluWavhkSOImuAAAAIDZEJy8zTdI2RY/SVKE5awkaVv8KQpEAAAAACZCcPI2i0XnrSGSpMDsnOA0eclO9Xz2a63aneDNngEAAAC4iODkZav3HNexzGqSpBqWc/bjiclpum/RdsITAAAAYAIEJy/Kthl6auXPSlKoJKm6LgWn3IV6sz7by7I9AAAAwMsITl60Lf60ElPSlWTkLNWrnmfGScoJTwnJadoWf9oLvQMAAACQi+DkRSfOpkmSkoycpXp5Z5wKawcAAADAOwhOXhQZGihJ9qV6NSyFB6fcdgAAAAC8g+DkRTGNIxQVFnBpxilfcLJIig4PVEzjCC/0DgAAAEAugpMX+Vgt+seQljqjnD1O4XmW6lku/nfGsNbysVoKeTYAAAAATyE4edmgNnXUKTpYkuNSvajwQM277UrFto32VtcAAAAAXOTr7Q5Aqls9RDopNQ3JlE5L9WsE6pu/Xc1MEwAAAGASzDiZQIZvzh6nUFuKJCn5QhahCQAAADARgpMJZPrm7HGypifJIpvOpmXpfHqWl3sFAAAAIBfByQQyfHJmnCyGTVEBmZKkxBTu3QQAAACYBcHJBGxWfxl+OQUimoVeDE7JBCcAAADALAhOZhFYXZLUODhdEsEJAAAAMBOCk1kE5dzktn7QxeDEUj0AAADANEoUnI4eParff//d/njbtm2aPHmy3nrrrTLrWFVjBFWXJNXzT5UkJSRf8GJvAAAAAORVouB06623au3atZKkxMREXXPNNdq2bZsee+wxzZ49u0w7WGVcnHGK9MsJTCzVAwAAAMyjRMFp9+7diomJkSQtXbpUbdu21ebNm/X+++9rwYIFZdm/KiN3xqmm5ZwkluoBAAAAZlKi4JSZmamAgABJ0po1azR8+HBJUsuWLZWQkFB2vatKLs44hecGJ2acAAAAANMoUXBq06aN3njjDW3YsEFxcXGKjY2VJB07dkw1a9Ys0w5WGRdnnEKyz0qSTp7LUHpWthc7BAAAACBXiYLTs88+qzfffFN9+/bVLbfcog4dOkiSPv30U/sSPrjHuDjj5J+ZLH/fnG/LiZR0b3YJAAAAwEW+JXlS3759dfLkSaWkpKhGjRr24/fcc4+Cg4PLrHNVysX7OFkunFZUWKCOnE5VYkqaGkQwngAAAIC3lWjG6cKFC0pPT7eHpsOHD+ull17S/v37FRkZ6fLrrF+/XsOGDVPdunVlsVi0YsWKYtuvW7dOFoulwFdiYmJJPoa5BOfMOOnCGUWFB0qSEtjnBAAAAJhCiYLTtddeq4ULF0qSkpKS1LVrV73wwgsaMWKE5s2b5/LrnD9/Xh06dNBrr73m1vvv379fCQkJ9i93wppZGf5hOX84m6hevvtklU3HCU4AAACAKZQoOG3fvl29evWSJC1btkx16tTR4cOHtXDhQv373/92+XUGDx6sp556Stddd51b7x8ZGamoqCj7l9Vaoo9hGtFJ38n3gxtzHmSmatLRh7Qx4AGF/7bKux0DAAAAIKmEe5xSU1MVGhoqSfryyy91/fXXy2q16qqrrtLhw4fLtIOFueKKK5Senq62bdtq5syZ6tGjR5Ft09PTlZ5+qchCSkqKpJyS6pmZmeXeV2dse1aoS/wrBY5H6bRu/HW6snZFy2j5Fy/0rHLK/Z6b4XtfVTDmnseYexbj7XmMuecx5p7FeHuOO2NsMQzDcPcN2rdvr/Hjx+u6665T27ZttWrVKnXr1k0//PCDhg4dWqI9RxaLRcuXL9eIESOKbLN//36tW7dOnTt3Vnp6ut5++22999572rp1q6688spCnzNz5kzNmjWrwPHFixd7v5CFYdPAPVMUmHlalkJO2ySl+UUors2/JEvFnlUDAAAAzCY1NVW33nqrkpOTFRYWVmzbEgWnZcuW6dZbb1V2drauvvpqxcXFSZLmzJmj9evX64svvnC7064Ep8L06dNHl112md57771Czxc249SgQQOdPHnS6eCUN8vhjfJdNMJpu6zbVsho2LP8O1QFZGZmKi4uTtdcc438/Py83Z0qgTH3PMbcsxhvz2PMPY8x9yzG23NSUlJUq1Ytl4JTiZbq3XDDDerZs6cSEhLs93CSpP79+7u9X6m0YmJitHHjxiLPBwQEKCAgoMBxPz8/71+IF0651Mz3winJ232tZEzx/a9iGHPPY8w9i/H2PMbc8xhzz2K8y58741ui4CTJXpjh999/lyTVr1/fKze/3bFjh6Kjoz3+vmUipE7ZtgMAAABQLkq0ccZms2n27NkKDw9Xw4YN1bBhQ1WvXl1PPvmkbDaby69z7tw57dixQzt27JAkxcfHa8eOHTpy5Igkafr06RozZoy9/UsvvaRPPvlEBw8e1O7duzV58mR9/fXXmjBhQkk+hvc17C4jtK6KWitpM6SMatFSw+4e7RYAAAAARyWacXrsscf0zjvv6JlnnrFXtNu4caNmzpyptLQ0/fOf/3Tpdb7//nv169fP/njKlCmSpLFjx2rBggVKSEiwhyhJysjI0MMPP6w//vhDwcHBat++vdasWePwGhWK1UfZA5+Wz0fjZMgiS54IlRs/97R/VB2tPt7pHwAAAABJJQxO//d//6e3335bw4cPtx9r37696tWrp/vvv9/l4NS3b18VV5tiwYIFDo8feeQRPfLIIyXpsmkZLf+i7xpPUpeTH0lnj9mPJ/nU1vQLo9UtpJc6erF/AAAAAEq4VO/06dNq2bJlgeMtW7bU6dOnS92pqiahehdlTfxRun2Fvez4gpava7UtRhsOntSWQ6eUbXO7+CEAAACAMlKi4NShQwe9+uqrBY6/+uqrat++fak7VSVZfaSm/aSIppKkfXt2SpK+2ndCt/znW/V89mut2p3gzR4CAAAAVVaJluo999xzGjp0qNasWaNu3bpJkrZs2aKjR49q5cqVZdrBquZ4wGWqowOKzjwqqbX9eGJymu5btF3zbrtSsW0raBVBAAAAoIIq0YxTnz599Msvv+i6665TUlKSkpKSdP3112vPnj1F3ogWzmXbDH15PFSS1NRyzOFc7kK9WZ/tZdkeAAAA4GElvo9T3bp1CxSB2Llzp9555x299dZbpe5YVbQt/rR+SouU/KQmloLL8gxJCclp2hZ/Wt2a1vR8BwEAAIAqqkQzTigfJ86m6ZCtriSpqfVYse0AAAAAeA7ByUQiQwN1yMgJTnUtpxWswgNSZGigJ7sFAAAAVHkEJxOJaRyh4PBaOmmESZIa51uuZ5EUHR6omMYRXugdAAAAUHW5tcfp+uuvL/Z8UlJSafpS5flYLZoxrLV+XVpXtSwpamo5pj1GY0k5oUmSZgxrLR+rpegXAQAAAFDm3ApO4eHhTs+PGTOmVB2q6mLbRuvoD+2k+J9z9jnZco5HhQdqxrDWlCIHAAAAvMCt4DR//vzy6gfyaNC8gxT/oW5pkqaXfsmppvfxfd0VXT3I210DAAAAqiT2OJlRrRaSpMj0o2oaGSJJ+vn4WW/2CAAAAKjSCE5mVKt5zn9PHVSbqGqSpL3HUrzYIQAAAKBqIziZUfWGko+/lJWmmIhUSdK+BIITAAAA4C0EJzOy+kg1mkiSeiV/pquse/XzsTNe7hQAAABQdblVHAIesvdTKek3SdJl+97SB/7SsbMRSv/pXwpof513+wYAAABUQQQns9n7qbR0jHJq6V0SpdOyfHyH5OsjtR7unb4BAAAAVRRL9czEli2tmqb8oUmS7Pe8XfX3nHYAAAAAPIbgZCaHN0spx4o8bZEhpfyR0w4AAACAxxCczOTccZea/XLooLJtBWelAAAAAJQPgpOZhNRxqdkTX59Uz2e/1qrdCeXcIQAAAAASwclcGnaXwupKshR62mZIx4ya2mZrqcTkNN23aDvhCQAAAPAAgpOZWH2k2GcvPnAMT7kr82Zl3i6brPbyEbM+28uyPQAAAKCcEZzMpvVw6aaFUli0w+HjitB9mZO12hZjP2ZISkhO07b40x7uJAAAAFC1EJzMqPVwafJubeyxQKdsoZKkRzLvdghNeZ04m+bJ3gEAAABVDsHJrKw+8mnSW+uN9pKkTtYDRTaNDA30VK8AAACAKongZGIxjSN0IKCNJKmT5ZcC5y2SosMDFdM4wsM9AwAAAKoWgpOJ+Vgt6t53qCTpSusB+Sjbfi63dMSMYa3lYy28Ch8AAACAskFwMrme3Xsp0zdE1Szpamk5Yj8eFR6oebddqdi20cU8GwAAAEBZIDiZndVHfg27SpJevOqCcieX/nv3VYQmAAAAwEMIThXBZVdJklqc+UYPRu7UVda92nTguJc7BQAAAFQdvt7uAFxg2HL++9sGPagNetBfOr3mTSn8XzmlywEAAACUK2aczG7vp9K6Zwocrp71p4ylY3LOAwAAAChXBCczs2VLq6ZJMgqcytnrZCj980eUnZXl6Z4BAAAAVQrBycwOb5ZSjhV52iIpIDVBk559Tat2J3iuXwAAAEAVQ3Ays3OuFYDwPX9C9y3aTngCAAAAygnBycxC6rjU7ISqS5JmfbZX2baCy/oAAAAAlA7BycwadpfC6ipnUV5BNkM6ZtTUNltLGZISktO0Lf60R7sIAAAAVAUEJzOz+kixz0qSjHzhKXdiaVbm7bLl+TaeOJvmse4BAAAAVQXByexaD5duWqiMYMdle6cUpvsyJ2u1LcbheGRooCd7BwAAAFQJBKeKoPVw+U7Zo/v9Zus7WwtJ0mfZ3R1Ck0VSdHigYhpHeKmTAAAAQOVFcKogfHx9Nfzam/SfrL9Ikob7bNJw60ZdZd0rq2ySpBnDWsvHWvh+KAAAAAAl5+vtDsB1sW2jFdWznmzbpFqWs/q3/+uSpGNGhHa1m65BbYd6uYcAAABA5URwqkj2fqortk1V/oLjUTqt6N1/07Zgf2VfPkwxjSOYeQIAAADKEMGporBlS6umqWB9PclqyamyV3/rbPVcX1N1woM1Y1hrxbaN9kZPAQAAgEqHPU4VxeHNUsqxIk9bLVJdyynFWH9WYnKa7lu0Xat2J3iwgwAAAEDlRXCqKM4dd6lZpJLsS/lmfbZX2bb8C/sAAAAAuIvgVFGE1HHeRtIJVZckGZISktO0Lf50+fUJAAAAqCIIThVFw+5SWF2pwA6nHDZDOmbU1DZbS4fjJ86meaBzAAAAQOVGcKoorD5S7LMXHziGJ+PiaryV2V0UY/3Zfl8nSYoMDfRQBwEAAIDKi+BUkbQeLt20UApzrJZnseQUhxjvu0of+D+ljQEPKNa6TdHhgYppHOGlzgIAAACVB8Gpomk9XJq8Wxr7uX5rNtY+25RXlE7rdb+XNKXez9oWf5oCEQAAAEApEZwqIquP1LC7Gp2IK3TLU+69b3scekGj/7NZPZ/9mtLkAAAAQCkQnCqqi/d1KrxUBPd1AgAAAMoSwami4r5OAAAAgMcQnCoq7usEAAAAeAzBqaLivk4AAACAxxCcKqoS3tepVrUAD3UQAAAAqDwIThWZG/d1GmTdJkl6+MOdFIkAAAAA3ERwquhcvK/TPL+XNMi6TcdTqLAHAAAAuIvgVBm4eF+nGX7vyXJx2R4V9gAAAADXEZwqCzfu60SFPQAAAMA9BKfKwsX7OtXRpbBEhT0AAADANQSnysLF+zo94feevVDEybPpLNcDAAAAXEBwqiyc3NcpVw2dtReKePJ/+9Tz2a8pFAEAAAA4QXCqLBzu61RMszyFIqyyKTGZKnsAAACAMwSnyiT3vk7BNYttlr9QhESVPQAAAKA4Xg1O69ev17Bhw1S3bl1ZLBatWLHC6XPWrVunK6+8UgEBAWrWrJkWLFhQ7v2sUFoPl2Kfcalpd8tuWWWzV9lbsCme8AQAAAAUwqvB6fz58+rQoYNee+01l9rHx8dr6NCh6tevn3bs2KHJkydr/PjxWr16dTn3tIIJjXap2QN+K7Qx4AF7sQj2PAEAAACF8/Xmmw8ePFiDBw92uf0bb7yhxo0b64UXXpAktWrVShs3btSLL76oQYMGlVc3K57cQhEpCZKKn0GK0mnN83tJ92VO1mpbjH3P07zbrlRsW9cCGAAAAFDZeTU4uWvLli0aMGCAw7FBgwZp8uTJRT4nPT1d6enp9scpKSmSpMzMTGVmZpZLP92R24ey7ovlmqfl89EdkiyyFBOerBbJZuQUi4hL7yybrLJImvXZHvVtXlM+1uKr9FU05TXeKBpj7nmMuWcx3p7HmHseY+5ZjLfnuDPGFsMwTLGpxWKxaPny5RoxYkSRbVq0aKE77rhD06dPtx9buXKlhg4dqtTUVAUFBRV4zsyZMzVr1qwCxxcvXqzg4OAy6btZRSd9p3a/v6+gzNPOG0uanXmbFmTHynZxBeeIhtnqE22okmUnAAAAQJKUmpqqW2+9VcnJyQoLCyu2bYWacSqJ6dOna8qUKfbHKSkpatCggQYOHOh0cDwhMzNTcXFxuuaaa+Tn51fGrz5Esv1D2euflc+mfzlt/YTfIo33XalZmWO02hajFYd99O2ZAP1jSEsNauPaDXbNrnzHG4VhzD2PMfcsxtvzGHPPY8w9i/H2nNzVaK6oUMEpKipKx48fdzh2/PhxhYWFFTrbJEkBAQEKCAgocNzPz89UF2L59cdPana15EJwkgrueTqekq5JH+ysdHuezPb9rwoYc89jzD2L8fY8xtzzGHPPYrzLnzvjW6Hu49StWzd99dVXDsfi4uLUrVs3L/WogsgtFiHna+7y3yCX+zwBAAAAXg5O586d044dO7Rjxw5JOeXGd+zYoSNHjkjKWWY3ZswYe/t7771Xv/76qx555BH9/PPPev3117V06VI99NBD3uh+xWH1kWKfvfjAtfBU13JK43xWOdzn6dtDp8q1mwAAAIBZeTU4ff/99+rYsaM6duwoSZoyZYo6duyoJ554QpKUkJBgD1GS1LhxY/3vf/9TXFycOnTooBdeeEFvv/02pchd0Xq4dNNCKcz15XZP+C1yuM/ThMXbuccTAAAAqiSv7nHq27eviivqt2DBgkKf8+OPP5Zjryqx1sOllkOlrW9Iqx916SkOe54uxHCPJwAAAFRJFWqPE8qA1Ufqeq/be57+6feufJUlQ9LfP9qlTQdPsucJAAAAVQbBqSoqwZ6nWpYUbQ2YoEHWbUq6kKnRb29Vz2e/ZukeAAAAqgSCU1VVgj1PETqreX4v2fc8JSan6b5F7HsCAABA5UdwqspaD5cm75YGPe1Sc8vFyaln/N5WN+tuWS5W3GPpHgAAACo7glNVV4I9TzUs5/Rf/6ftFfdYugcAAIDKjuCEfHueXJdbcS936V5CcpruXbRdL6/5hdknAAAAVCoEJ+TI3fMUXNPlp1jzLd2zyiZJenHNAfV4htknAAAAVB4EJ1zSerg05WcpuJbLTyls6Z4kJaZQOAIAAACVB8EJjnz9pb+8qJz9Ts73POWVu3Rvks9HslI4AgAAAJUIwQkFlaBUuZQz+2S1SA/7faRNAZM0yecj9U7/Rq+8O1+9n4lj9gkAAAAVlq+3OwCTaj1cajlUit8gLRsnXTjj1tOjdEYP+31kf3wsPUKzFo/Rd91u0oDWUYppHCEfq3szWgAAAIC3MOOEoll9pKZ9pWH/lrtL9yz5muYu4/t9y1Ld8p9vKV0OAACACoXgBOdKuHQvr/wV+I4np+reRdv15Gd7tOXQKfZAAQAAwNRYqgfXlHLpnnSxAp9yKvAdMyI0K3OM3tkkvbPpN0WHB2rGsNaKbVvycAYAAACUF2ac4LoCS/dKLnfp3j98F+oq616dSE6lfDkAAABMi+AE9+Uu3Qst3dI9q0Ua77tKH/g/pQ0BD2igdZseXb5LGVm2MuwsAAAAUHoEJ5RM6+HSQ3ukvo+WycvlzkCNTvtA3Z/+kpknAAAAmAp7nFByVh+p7zQpspW0apqUcsx+ypB7i/lyi0c87PeRbs3+Wov/e7VSW7TT5c1aqGXXQfLx5VIFAACA9/DTKEovt3DE4c3SuePSqUPSDwuks8ecPrUw9ntAxX8kxUvH42rqu5aPKLvlMEWGBnIPKAAAAHgcwQllw+ojNe5lf2jpPdVegc+4cMat2af894CqbZzSkH3T9O7utfqvrbOOhnTQ48PbUYEPAAAAHsMeJ5SPPBX4LLLIKEUVvvyFJD5M/6uWL35DL6/5hfs/AQAAwCMITihfFyvwWUpx89z8cgtJZK19Rr3mxFFIAgAAAOWOpXoof3n3QO1fKX37utvFI/JyKCSRQSEJAAAAlD9+woRn5O6BatxLuqybLF88Ip0t/UxRwUISEfq1wQ3yq9NcQTXqEaQAAABQJviJEp6XOwO1/nkZ656WVPLZp/yFJCKN06pz9C3paM7j43ERSmk9Ws1bXSGF1JHqdilxtwEAAFB1EZzgHRfvAWWJbCUj/z2gjIKByFWFBqm9r0h7cx77BtVUm5DOshwOk5r0zukHAAAA4ATBCd7Vergsee4BdWDfDoXtXaw6OlUmL58/SFkunFKzC6ulRatlhEbL0ukOqWbTnNmoht0JUgAAACgUwQnel+ceUM3b3aDsrJnas3W1An5dpaaHFspQOZV/PJsgXVwqKElGWF1ZYp/NWUoIAAAA5EE5cpiOj6+v2vQYqma3vyLLTe/JElo+N7rNvxrQSDkmY+nt0qrpOTfvtWWXy/sCAACg4mHGCeaWu5SvDApJOGP/LcK3r+eUTA+uKUv7UdLlQ6QGXaWjW6Vzx1nWBwAAUAURnGB+5VRIwhlL6qlLIcpilcWwXToZGi2xPwoAAKDKIDih4nChkES5Bam8oUmScTZBljz7oxRcU8qdnSJEAQAAVDoEJ1QsRRSSuHDmD2UcP6imR5eVS5DK/xIFXjLP7BSzUQAAAJUPwQkVWm4hiVzZWU9rz9bV+vngL/rtl9261fdrReu0R/tUYDaKIAUAAFDhEZxQqeQGqTY9hmrV7gTd8OkuNTi3UwMsP+g6342qaTlrb1tey/oKVOsrLkgF18rpxPk/CVUAAAAmRnBCpRXbNlrXtI7StvhOits7RIN+PKpmabsUqSQ1tCR6bDbKaZDKi9kpAAAAUyI4oVLzsVrUrWlNdWtaU48Nba0tBzvqyw1bZW3QQiO/G6XLzufMRt3l+0XOjXbLq9Z5HsW+Rb6b8hKkAAAAzIHghCrDx2pR18YROrXP0JB+TfXAgMvts1H3brlcM/wWqm4RM1DlWfa8WPmDFNX7AAAAvILghCor72zUqsb36sZPexS5H8omq3x0qSS514JUcdX7uEkvAABAuSE4ASp+P9QJVdf3thbqbP3F4/ujipV/NspidbzfFEUoAAAAygzBCbgo/36obfFddOJsmn47marfth3Rtymt7W1fyx6hGOvPHq3W51S+m/QWCFZ5sXcKAADALQQnoBC5ISrXxKubaVv8acXtTdS7m36TIau+tbXWt2qtp7NHK8b6c5GzUV4LUsUprggFs1MAAAAFEJwAF+SdjYppHKFZn+1VQnKapJz9T9/aCs5GVegglRd7qQAAAAhOgLsu7Yc6rRNn01SrWoBkkU6eS9dvJ1P13yKW9bmyP6pChKr8e6nyVvorLFQBAABUAgQnoATyL+XLK3dZX+7+qOKC1J8KkyTVVkrFmZ3Kv5cqb6W/QgpUWDuOUb3TKbIcDpMa9WC2CgAAVEgEJ6CMFbU/qqgglZc7s1OmVEiBCp/1z6qzJB2eV/xsFSEKAACYGMEJKGfFFZpYseOYTp/PsJ8rar+UK9X7TDk7lV9xs1XF7aWiYAUAAPAyghPgYQXLnjvORiWmpNnb2tyo3meam/SWlLO9VHm5U07dli0d3szyQAAAUCoEJ8CLXFnWlxukiqve5+5NeitEqCoqNEnFl1PPO1u1f6X001Ip9eSltmF1pdhnpdbDy6/vAACg0iE4ASZSmmV9kooMVhWyCIU73JmtSjkmLb1duur+gpUAWRIIAACKQHACTMydZX35FRascpWmRHqFCFnFzVblKqoSYF7O7mFVXOjiflcAAFQqBCeggnBnWZ8kWS2SzSj8tYpb9udsdir/Xqq8KkSoys+dJYH5Q1ZxoauQ0uwu78sCAACmQ3ACKqjiglRkaKA6NayhHw6fKXSZX36uzk7l3UtVWKU/ZwUqKmSwyit/SCoudBVSmt0hhBV34+DiZqsodgEAgFcQnIBKorCb8uZf5he3N1HvbvpNFklFTEYVUNReqvyV/lwpUFHcbFWVU9yNg4u631VgdWn7gpx9WrkKWU5oObxJ9U5vkSW+muTrx54tAADKAMEJqALy7pWKaRyhWZ/tVUKya8v6iuNOgQpns1WVbnbKHc5msnJDVmEKWU7oa9gu3XQ4L3fuleXOLBgAAFUAwQmoYmLbRuua1lElXtbnDldnq6rEXipPKc2erbyczYLlDWHuFMbIv9SQgAYAqCAITkAV5MqyvpIUnXCHO/elcrecOsHKRe7s0XK2ZyuvokJWWlLB+2qVZUBzp7Q8e8UAAG4iOAFwUJqiE6UJVc6W/eXlrJx6/tmqk0aYdtiaqL91h4yL/cxFyCoH7oSssgxorpaWP3Wo4F4xd4p15AtoluzsnD1lh8OkRj0o7AEAlRTBCUCxXJ2dyh+q3C1C4Q53ZqtOqLq22VrKJqsGWbdpht9C1S3hkkBDUt6Mlf8xPMydKofFBTDJvWId+R77Spf2lLkz21aW9wkrzX610iytLKvwV1yoLOwcAHgBwQlAiRUXqsqyCIUzrs5WrbbFKC69c4mXBNoMq3wstiIf52VYrLLk+QGakFWBOAtkpSlDX9y50twnrDRt3aniWFRbZzN1xYW5wmYAnQROa8cxqnc6peAsn6dCY2mKqTibeXRnZtJTs5i2bFkOb7w0s9qkd8WeLa3os78e/L5X6HEqBxbDMMrjF8KmlZKSovDwcCUnJyssLMzb3VFmZqZWrlypIUOGyM/Pz9vdqfQYb8/KthnacvCEvtywVQN7dVVMk9rlUoSivFhlK3Ymq6jQdULVtd1ooSst+UqzW04X+j7MZKFSKE2YK8v3dadPpQmN7ryPqzOP7iwjLeMlp24F2/J4n7KcOS2vz+NOIRzJMXi4+dmzsrO1Y1Ocrug1SL55fzlQ3C8a3N0LWlyoL8v3cXfcPMydbEBw8jJ+kPcsxtvzihrzbJvhkSIUZpEbwgorxZ6db+Yq/+O8CFlAFeZO4HQ3NJrhfcoyBBf33OKU5nWdheCyeh93lOaXBWX5PsW1DasrxT4rtR5esvcuJXeyAUv1AHiFt4pQeEvuckJnNw52dr+rBCNC/83qp8NGtBpaEnWb31rV0Sn7+fxLBAFUIu783S7LJaeeep/StPXU5ynunLO9lGX1Pu4ozX0Cy/J9ijuXkiAtHSPdtNBr4clVBCcAplCSIhQVdbbKlT1ZRYWs3EIXufIXxsi7RPBPhSkswEeDGvmoc2iSGsQvleXspd8mFhuy3PntIQAAJXZx7cSqv0sth5p6HxXBCYDpFRWqchU1W3XibJpqVQuQLNLJc+mFhiyzKyxkOTv/rZHn8QVp9b6cP/paOquzxXnIurxZCzXv1F8HfvhKF878oaAa9RweR2b8USCEOSivZTEAgErKkFL+yNlj1biXtztTJIITgArPWbDKK2/IqohBqjSyDKtjqFIRIWufZP38q4uzdvUk5X9cT3VDu2tK29NqFHhWAeFRkqT05EQF1ainll0GyOePbcVsxK4ltb/J+UbsstwjAAAwv3PHvd2DYhGcAFQpRe2tqix7qcpK/s+d//Gxs1ma+n2YpLwbaXNCVtQ3G3RLzGVqVKuJIhv0U6fuU3Tguy914cwfCqheVymRXXQyNUuRtkB1kq9+sLXWiewmBR836KeYng/L5+iW0lXkKrQ6VF2p07jCq5x5ahM6AMBRSB1v96BYBCcAVVpukKqMe6m8JTElXS+uOWB/fGmc6l088n0h5wp/HBUWYA9htWwXl11mpyvSFqgYq5988i/pKGqJR++pRd+PZOBTxd/ktSzLBufOtrlS7toM1cicvRYAlAlLTnU9k9/g2hTB6bXXXtPcuXOVmJioDh066JVXXlFMTEyhbRcsWKA77rjD4VhAQIDS0qrGUhsA5aeke6lcma2qyiGruM/tbGYrfwjL61Koqlbs3rbI0EDFNI4oGLJyWX0KBi43HhuZmfpj3zl1aNhT8vVzbFtcYMt/ztv3v9m/spB7DF2cmavZ1HnbUpVxzvM+pQ2cxb1PeYVGlo0CpXDxphqxz5i6MIRkguC0ZMkSTZkyRW+88Ya6du2ql156SYMGDdL+/fsVGRlZ6HPCwsK0f/9++2OLhbuYACh/Ja38R8gqH0XPbBXkTsjK/72LaRwhH2sJ/z9TWCgr7pyrM2hl3bZxr5yv/LNvhd2Ysqi2ZXkDTCeBM+vX9dqxYXXBWT5PhEZ338fdIOjOMlJPLTkNravsjrcrft8ONT3/vSypp4pua8aZ07Ict+Je1x3l+YuGkr5PfuX1PoXex+kZ05cil0xwA9yuXbuqS5cuevXVVyVJNptNDRo00KRJk/T3v/+9QPsFCxZo8uTJSkpKKtH7cQPcqo3x9jzGvKC8N/8tbchCyRQ3jkUvFyw8dGVlZ+mrTds0sFdXxTSpXWToyv99L1Ugq+LK5d8VW7bz0Fger+3OueICp7O2pQy2mdm2nDGPHSS/Y9+Vz/uUxcypu0HdnXEr7nWL20tZs6nbn73IJcCFXZt5P4Or7+PKLwvK4n1K8osTD3MnG3g1OGVkZCg4OFjLli3TiBEj7MfHjh2rpKQkffLJJwWes2DBAo0fP1716tWTzWbTlVdeqaefflpt2rQp9D3S09OVnp5uf5ySkqIGDRro5MmTpglOcXFxuuaaa/ih0gMYb89jzEsn22bo+8NndOJsuiJDA9SxQXX9eDRJJ86m6/CpVC35/nclplz6N45gVf7cCV11Qv01qnMDpaRl6tOdCTqdmlngXKNawQW+t5GhAercsAbBqgj8u+J5jLkLbNmy5BazCakjo0G3EgcCj4x3Gfa3IktJSVGtWrXMH5yOHTumevXqafPmzerWrZv9+COPPKJvvvlGW7duLfCcLVu26MCBA2rfvr2Sk5P1/PPPa/369dqzZ4/q169foP3MmTM1a9asAscXL16s4ODgsv1AAOBhNkM6lGJRSqYU5ic1DjUUf9aiXWekH/606lzWpR+8LTJkqOjHji7ekLDIx1VZcWPhzrg5nsv//Qj3M9S9jk21g6QQ35yWZ7Nyvs9NwwzlzVT5r4P85wEAhUtNTdWtt95aOYNTfpmZmWrVqpVuueUWPfnkkwXOM+OEvBhvz2PMPS93zK/uP0A7j50rdLYq7+M1+04UmA1h35W55Z2tKmzmsUawn67tEK0BrSKL/L4X9bgizHTx74rnMeaexXh7jjszTl4tDlGrVi35+Pjo+HHHm10dP35cUVFRLr2Gn5+fOnbsqIMHDxZ6PiAgQAEBAYU+z0wXotn6U9kx3p7HmHteYIC/erZwvCdGYY97tqijx4e1dWnfVVW8cbAZHT+boX+vPVTk+TOpmVqw5YgWbDniNATnfxxRzU/XXVFPA1pHFXsdlGtRDRfx74rnMeaexXiXP3fG16vByd/fX506ddJXX31l3+Nks9n01VdfaeLEiS69RnZ2tnbt2qUhQ4aUY08BoHJzVoo9/+O8pdnz/gDt7v2umNkqf85Kvud/fPp8pt7Z9Jve2fSbW9+f4opqOAtZ7gS0mMYRDu9L0Q0AnuL1cuRTpkzR2LFj1blzZ8XExOill17S+fPn7fdqGjNmjOrVq6c5c+ZIkmbPnq2rrrpKzZo1U1JSkubOnavDhw9r/Pjx3vwYAFClFBa0chV1vytXZimY2TIXZyGruHPF3YPL3VmwvKLCAnRTp/pKOmnRr2sPacn3fzhcI67OmBGyALjL68Fp1KhR+vPPP/XEE08oMTFRV1xxhVatWqU6dXKWkxw5ckRWq9Xe/syZM7r77ruVmJioGjVqqFOnTtq8ebNat27trY8AAMjDldmr4s6Vx8wWzMXdWbC8ElPSLy5T9JEOFFyu6M6MWUlnxQq9wTIBDKj0vB6cJGnixIlFLs1bt26dw+MXX3xRL774ogd6BQDwtrKY2Srt8kFUXM4CWWlmxfIq7l5fpZl1JZQB5mKK4AQAgLvcmdkq6fJBQlfV5e4sWN4AVpq9YeVVrINABpQewQkAUOmVZvmgs9CVlZ2lrzZt08BeXRXTpHaxoSv3h+CwIH+WGlZipdkbVl7FOkq7THFb/Gn9cNKiGodOycfXt0yqKeYv7EG4g9kRnAAAKIaz0JWZmamk/Ya6No6Qn6+12NCV9wfB4gIZhTFQmLIMZCVbpuijhQd+KLZtccsW3V1CW9xsW3GhikqLKC8EJwAAyklxe7ScBTJ3CmPk/rCafCFTK3Yc0+nzGfZzpalgh6qpLKspunN9uTPbVlRAi9ubWODvgKthjqIgcIbgBACACblTGCPvD3CPDW3t8s2Mnf3AWZqlYYBUdtdEaQKaO23NUBQkdwnwDyctqhl/Wt2aRRLQTILgBABABVOamayiHndrWtOt0OVuUY28mAVDWXPneinNEse8yrMoSA4fLTzwfZkGNHeWOBb33Kq6HJLgBAAAJJUsdBV1rqilhu7MgpX3MkWgLJXlHrS8yjKgFVUUxJU9Z8X9XSvNcsiKFLoITgAAoMwVNysmuTYLVpiJVzfTloMn9OWGrRrYq6vDMiZ3ZsxKOysGeEtZ7kFz532Ke25plkNGhwdqxrDWim0b7VK/vIngBAAAKgwfq0VdG0fo1L6cSoZ5f1Pt7oxZSWfF3P1NfVntqQEqCrfCXHKa7lu0XfNuu9L04YngBAAAqqTSzorlKq60fEmLBJR1sQ4CGczKkGSRNOuzvbqmdZSpl+0RnAAAAEqhLPeG5T4uy2Id5blMsSwDGeGu6jIkJSSnaVv86WL/rngbwQkAAMCEyiOQ5XJ3meK2X//Ulxu2akCPGPn4+rpcTbEk5bxdmW1D5XTirLlv+k1wAgAAqGLcXaaYu6+sW9Oa8vPzK7JtccsWXbmBrCuzba4EtIhqfrruinoKC/J3K8yxxNG7IkMDvd2FYhGcAAAAUCZcmSUri9dxtq8sbyhzJ8xRFMQ7LJKiwnO+b2ZGcAIAAECF4k5AczfMebMoSG7brOwsfbVpm2o0aKEl3/9RZgHNHZ4Kc7lzjjOGtTZ1YQiJ4AQAAAC4rTz3oGVmZippv6Eh/ZrqgQGXl0lAc1YUpLib2Bb23LJaDhnFfZwAAAAAlFZZBrTiioIUteessOeW5XLIwt7XrAhOAAAAQBXgrChISZ9b2uWQFYXV2x0AAAAAALMjOAEAAACAEwQnAAAAAHCC4AQAAAAAThCcAAAAAMAJghMAAAAAOEFwAgAAAAAnCE4AAAAA4ATBCQAAAACcIDgBAAAAgBO+3u6ApxmGIUlKSUnxck9yZGZmKjU1VSkpKfLz8/N2dyo9xtvzGHPPY8w9i/H2PMbc8xhzz2K8PSc3E+RmhOJUueB09uxZSVKDBg283BMAAAAAZnD27FmFh4cX28ZiuBKvKhGbzaZjx44pNDRUFovF291RSkqKGjRooKNHjyosLMzb3an0GG/PY8w9jzH3LMbb8xhzz2PMPYvx9hzDMHT27FnVrVtXVmvxu5iq3IyT1WpV/fr1vd2NAsLCwviL4UGMt+cx5p7HmHsW4+15jLnnMeaexXh7hrOZplwUhwAAAAAAJwhOAAAAAOAEwcnLAgICNGPGDAUEBHi7K1UC4+15jLnnMeaexXh7HmPueYy5ZzHe5lTlikMAAAAAgLuYcQIAAAAAJwhOAAAAAOAEwQkAAAAAnCA4AQAAAIATBCcveu2119SoUSMFBgaqa9eu2rZtm7e7VCnMmTNHXbp0UWhoqCIjIzVixAjt37/foU3fvn1lsVgcvu69914v9bjimzlzZoHxbNmypf18WlqaJkyYoJo1ayokJEQjR47U8ePHvdjjiq9Ro0YFxtxisWjChAmSuMbLwvr16zVs2DDVrVtXFotFK1ascDhvGIaeeOIJRUdHKygoSAMGDNCBAwcc2pw+fVqjR49WWFiYqlevrrvuukvnzp3z4KeoOIob78zMTE2bNk3t2rVTtWrVVLduXY0ZM0bHjh1zeI3C/l4888wzHv4kFYeza3zcuHEFxjM2NtahDde4e5yNeWH/rlssFs2dO9fehuvcewhOXrJkyRJNmTJFM2bM0Pbt29WhQwcNGjRIJ06c8HbXKrxvvvlGEyZM0Lfffqu4uDhlZmZq4MCBOn/+vEO7u+++WwkJCfav5557zks9rhzatGnjMJ4bN260n3vooYf02Wef6cMPP9Q333yjY8eO6frrr/dibyu+7777zmG84+LiJEk33nijvQ3XeOmcP39eHTp00GuvvVbo+eeee07//ve/9cYbb2jr1q2qVq2aBg0apLS0NHub0aNHa8+ePYqLi9Pnn3+u9evX65577vHUR6hQihvv1NRUbd++XY8//ri2b9+ujz/+WPv379fw4cMLtJ09e7bDdT9p0iRPdL9CcnaNS1JsbKzDeP73v/91OM817h5nY553rBMSEvTuu+/KYrFo5MiRDu24zr3EgFfExMQYEyZMsD/Ozs426tata8yZM8eLvaqcTpw4YUgyvvnmG/uxPn36GA8++KD3OlXJzJgxw+jQoUOh55KSkgw/Pz/jww8/tB/bt2+fIcnYsmWLh3pY+T344ING06ZNDZvNZhgG13hZk2QsX77c/thmsxlRUVHG3Llz7ceSkpKMgIAA47///a9hGIaxd+9eQ5Lx3Xff2dt88cUXhsViMf744w+P9b0iyj/ehdm2bZshyTh8+LD9WMOGDY0XX3yxfDtXSRU25mPHjjWuvfbaIp/DNV46rlzn1157rXH11Vc7HOM69x5mnLwgIyNDP/zwgwYMGGA/ZrVaNWDAAG3ZssWLPauckpOTJUkREREOx99//33VqlVLbdu21fTp05WamuqN7lUaBw4cUN26ddWkSRONHj1aR44ckST98MMPyszMdLjeW7Zsqcsuu4zrvYxkZGRo0aJFuvPOO2WxWOzHucbLT3x8vBITEx2u6/DwcHXt2tV+XW/ZskXVq1dX586d7W0GDBggq9WqrVu3erzPlU1ycrIsFouqV6/ucPyZZ55RzZo11bFjR82dO1dZWVne6WAlsW7dOkVGRuryyy/Xfffdp1OnTtnPcY2Xr+PHj+t///uf7rrrrgLnuM69w9fbHaiKTp48qezsbNWpU8fheJ06dfTzzz97qVeVk81m0+TJk9WjRw+1bdvWfvzWW29Vw4YNVbduXf3000+aNm2a9u/fr48//tiLva24unbtqgULFujyyy9XQkKCZs2apV69emn37t1KTEyUv79/gR9u6tSpo8TERO90uJJZsWKFkpKSNG7cOPsxrvHylXvtFvbveO65xMRERUZGOpz39fVVREQE134ppaWladq0abrlllsUFhZmP/7AAw/oyiuvVEREhDZv3qzp06crISFB//rXv7zY24orNjZW119/vRo3bqxDhw7p0Ucf1eDBg7Vlyxb5+PhwjZez//u//1NoaGiBpe1c595DcEKlNmHCBO3evdthv40kh/XX7dq1U3R0tPr3769Dhw6padOmnu5mhTd48GD7n9u3b6+uXbuqYcOGWrp0qYKCgrzYs6rhnXfe0eDBg1W3bl37Ma5xVFaZmZm66aabZBiG5s2b53BuypQp9j+3b99e/v7++utf/6o5c+YoICDA012t8G6++Wb7n9u1a6f27duradOmWrdunfr37+/FnlUN7777rkaPHq3AwECH41zn3sNSPS+oVauWfHx8ClQVO378uKKiorzUq8pn4sSJ+vzzz7V27VrVr1+/2LZdu3aVJB08eNATXav0qlevrhYtWujgwYOKiopSRkaGkpKSHNpwvZeNw4cPa82aNRo/fnyx7bjGy1butVvcv+NRUVEFCv5kZWXp9OnTXPsllBuaDh8+rLi4OIfZpsJ07dpVWVlZ+u233zzTwUquSZMmqlWrlv3fEa7x8rNhwwbt37/f6b/tEte5JxGcvMDf31+dOnXSV199ZT9ms9n01VdfqVu3bl7sWeVgGIYmTpyo5cuX6+uvv1bjxo2dPmfHjh2SpOjo6HLuXdVw7tw5HTp0SNHR0erUqZP8/Pwcrvf9+/fryJEjXO9lYP78+YqMjNTQoUOLbcc1XrYaN26sqKgoh+s6JSVFW7dutV/X3bp1U1JSkn744Qd7m6+//lo2m80eZOG63NB04MABrVmzRjVr1nT6nB07dshqtRZYToaS+f3333Xq1Cn7vyNc4+XnnXfeUadOndShQwenbbnOPYelel4yZcoUjR07Vp07d1ZMTIxeeuklnT9/XnfccYe3u1bhTZgwQYsXL9Ynn3yi0NBQ+zrr8PBwBQUF6dChQ1q8eLGGDBmimjVr6qefftJDDz2k3r17q3379l7ufcU0depUDRs2TA0bNtSxY8c0Y8YM+fj46JZbblF4eLjuuusuTZkyRREREQoLC9OkSZPUrVs3XXXVVd7ueoVms9k0f/58jR07Vr6+l/455xovG+fOnXOYoYuPj9eOHTsUERGhyy67TJMnT9ZTTz2l5s2bq3Hjxnr88cdVt25djRgxQpLUqlUrxcbG6u6779Ybb7yhzMxMTZw4UTfffLPDskrkKG68o6OjdcMNN2j79u36/PPPlZ2dbf+3PSIiQv7+/tqyZYu2bt2qfv36KTQ0VFu2bNFDDz2k2267TTVq1PDWxzK14sY8IiJCs2bN0siRIxUVFaVDhw7pkUceUbNmzTRo0CBJXOMl4ezfFSnnlzAffvihXnjhhQLP5zr3Mm+X9avKXnnlFeOyyy4z/P39jZiYGOPbb7/1dpcqBUmFfs2fP98wDMM4cuSI0bt3byMiIsIICAgwmjVrZvztb38zkpOTvdvxCmzUqFFGdHS04e/vb9SrV88YNWqUcfDgQfv5CxcuGPfff79Ro0YNIzg42LjuuuuMhIQEL/a4cli9erUhydi/f7/Dca7xsrF27dpC/y0ZO3asYRg5Jckff/xxo06dOkZAQIDRv3//At+LU6dOGbfccosREhJihIWFGXfccYdx9uxZL3wa8ytuvOPj44v8t33t2rWGYRjGDz/8YHTt2tUIDw83AgMDjVatWhlPP/20kZaW5t0PZmLFjXlqaqoxcOBAo3bt2oafn5/RsGFD4+677zYSExMdXoNr3D3O/l0xDMN48803jaCgICMpKanA87nOvctiGIZR7ukMAAAAACow9jgBAAAAgBMEJwAAAABwguAEAAAAAE4QnAAAAADACYITAAAAADhBcAIAAAAAJwhOAAAAAOAEwQkAAAAAnCA4AQBQDIvFohUrVni7GwAALyM4AQBMa9y4cbJYLAW+YmNjvd01AEAV4+vtDgAAUJzY2FjNnz/f4VhAQICXegMAqKqYcQIAmFpAQICioqIcvmrUqCEpZxndvHnzNHjwYAUFBalJkyZatmyZw/N37dqlq6++WkFBQapZs6buuecenTt3zqHNu+++qzZt2iggIEDR0dGaOHGiw/mTJ0/quuuuU3BwsJo3b65PP/3Ufu7MmTMaPXq0ateuraCgIDVv3rxA0AMAVHwEJwBAhfb4449r5MiR2rlzp0aPHq2bb75Z+/btkySdP39egwYNUo0aNfTdd9/pww8/1Jo1axyC0bx58zRhwgTdc8892rVrlz799FM1a9bM4T1mzZqlm266ST/99JOGDBmi0aNH6/Tp0/b337t3r7744gvt27dP8+bNU61atTw3AAAAj7AYhmF4uxMAABRm3LhxWrRokQIDAx2OP/roo3r00UdlsVh07733at68efZzV111la688kq9/vrr+s9//qNp06bp6NGjqlatmiRp5cqVGjZsmI4dO6Y6deqoXr16uuOOO/TUU08V2geLxaJ//OMfevLJJyXlhLGQkBB98cUXio2N1fDhw1WrVi29++675TQKAAAzYI8TAMDU+vXr5xCMJCkiIsL+527dujmc69atm3bs2CFJ2rdvnzp06GAPTZLUo0cP2Ww27d+/XxaLRceOHVP//v2L7UP79u3tf65WrZrCwsJ04sQJSdJ9992nkSNHavv27Ro4cKBGjBih7t27l+izAgDMi+AEADC1atWqFVg6V1aCgoJcaufn5+fw2GKxyGazSZIGDx6sw4cPa+XKlYqLi1P//v01YcIEPf/882XeXwCA97DHCQBQoX377bcFHrdq1UqS1KpVK+3cuVPnz5+3n9+0aZOsVqsuv/xyhYaGqlGjRvrqq69K1YfatWtr7NixWrRokV566SW99dZbpXo9AID5MOMEADC19PR0JSYmOhzz9fW1F2D48MMP1blzZ/Xs2VPvv/++tm3bpnfeeUeSNHr0aM2YMUNjx47VzJkz9eeff2rSpEm6/fbbVadOHUnSzJkzde+99yoyMlKDBw/W2bNntWnTJk2aNMml/j3xxBPq1KmT2rRpo/T0dH3++ef24AYAqDwITgAAU1u1apWio6Mdjl1++eX6+eefJeVUvPvggw90//33Kzo6Wv/973/VunVrSVJwcLBWr16tBx98UF26dFFwcLBGjhypf/3rX/bXGjt2rNLS0vTiiy9q6tSpqlWrlm644QaX++fv76/p06frt99+U1BQkHr16qUPPvigDD45AMBMqKoHAKiwLBaLli9frhEjRni7KwCASo49TgAAAADgBMEJAAAAAJxgjxMAoMJitTkAwFOYcQIAAAAAJwhOAAAAAOAEwQkAAAAAnCA4AQAAAIATBCcAAAAAcILgBAAAAABOEJwAAAAAwAmCEwAAAAA48f84XE7ug9O27wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Function: add x mul INT - 1 mul x add INT + 5 x\n",
      "Predicted Expression: add x INT + 2 x x INT + 2 mul div + 1 pow mul m\n",
      "Target Expression: add mul INT - 1 pow x INT + 2 mul INT - 4 x\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"best_lstm_model.pth\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # working locally on mac, change to 'gpu' if working on a HPC\n",
    "model = Seq2SeqLSTM(vocab_size).to(device)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "example_idx = 4\n",
    "input_function = test_f[example_idx]\n",
    "input_target = test_taylor_f[example_idx]\n",
    "target_value = tokenizer.decode(input_target.cpu().numpy())\n",
    "function_value = tokenizer.decode(input_function.cpu().numpy())\n",
    "\n",
    "encoded_function = input_function.unsqueeze(0).to(device)\n",
    "encoded_target = input_target.unsqueeze(0).to(device)\n",
    "tgt_input = encoded_target[:, :-1]\n",
    "tgt_output = encoded_target[:, 1:]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(encoded_function, tgt_input)\n",
    "    \n",
    "predicted_ids = outputs.argmax(dim=-1)[0].cpu().numpy()\n",
    "predicted_output = tokenizer.decode(predicted_ids)\n",
    "\n",
    "print(f\"Input Function: {function_value}\")\n",
    "print(f\"Predicted Expression: {tokenizer.decode(encoded_target[:,0].cpu().numpy())}\" + \" \" +  predicted_output[:len(target_value)])\n",
    "print(f\"Target Expression: {target_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=2, dim_feedforward=1024, max_seq_len=512, dropout=0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, max_seq_len, d_model))\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_layers, \n",
    "            num_decoder_layers=num_layers, \n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "\n",
    "        output = self.transformer(src_emb, tgt_emb)\n",
    "        return self.output_layer(output)\n",
    "    \n",
    "def encode_sequence(sequence, tokenizer, max_len=512):\n",
    "    tokens = tokenizer.encode(sequence).ids\n",
    "    tokens = tokens[:max_len] + [0] * (max_len - len(tokens))\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "def train_transformer(model, train_loader, val_loader, epochs, criterion, optimizer, device, save_path, clip_grad=1.0):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), tgt_output.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = evaluate_transformer(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Best model saved at epoch {epoch+1} with validation loss {best_val_loss:.4f}\")\n",
    "\n",
    "def evaluate_transformer(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), tgt_output.contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch 1/20 - Train Loss: 0.2650 - Val Loss: 0.1679\n",
      "Best model saved at epoch 1 with validation loss 0.1679\n",
      "Epoch 2/20 - Train Loss: 0.1515 - Val Loss: 0.1445\n",
      "Best model saved at epoch 2 with validation loss 0.1445\n",
      "Epoch 3/20 - Train Loss: 0.1288 - Val Loss: 0.1184\n",
      "Best model saved at epoch 3 with validation loss 0.1184\n",
      "Epoch 4/20 - Train Loss: 0.1030 - Val Loss: 0.0955\n",
      "Best model saved at epoch 4 with validation loss 0.0955\n",
      "Epoch 5/20 - Train Loss: 0.0851 - Val Loss: 0.0797\n",
      "Best model saved at epoch 5 with validation loss 0.0797\n",
      "Epoch 6/20 - Train Loss: 0.0713 - Val Loss: 0.0672\n",
      "Best model saved at epoch 6 with validation loss 0.0672\n",
      "Epoch 7/20 - Train Loss: 0.0600 - Val Loss: 0.0567\n",
      "Best model saved at epoch 7 with validation loss 0.0567\n",
      "Epoch 8/20 - Train Loss: 0.0505 - Val Loss: 0.0454\n",
      "Best model saved at epoch 8 with validation loss 0.0454\n",
      "Epoch 9/20 - Train Loss: 0.0418 - Val Loss: 0.0374\n",
      "Best model saved at epoch 9 with validation loss 0.0374\n",
      "Epoch 10/20 - Train Loss: 0.0354 - Val Loss: 0.0315\n",
      "Best model saved at epoch 10 with validation loss 0.0315\n",
      "Epoch 11/20 - Train Loss: 0.0300 - Val Loss: 0.0265\n",
      "Best model saved at epoch 11 with validation loss 0.0265\n",
      "Epoch 12/20 - Train Loss: 0.0261 - Val Loss: 0.0233\n",
      "Best model saved at epoch 12 with validation loss 0.0233\n",
      "Epoch 13/20 - Train Loss: 0.0233 - Val Loss: 0.0213\n",
      "Best model saved at epoch 13 with validation loss 0.0213\n",
      "Epoch 14/20 - Train Loss: 0.0210 - Val Loss: 0.0193\n",
      "Best model saved at epoch 14 with validation loss 0.0193\n",
      "Epoch 15/20 - Train Loss: 0.0193 - Val Loss: 0.0180\n",
      "Best model saved at epoch 15 with validation loss 0.0180\n",
      "Epoch 16/20 - Train Loss: 0.0175 - Val Loss: 0.0168\n",
      "Best model saved at epoch 16 with validation loss 0.0168\n",
      "Epoch 17/20 - Train Loss: 0.0162 - Val Loss: 0.0158\n",
      "Best model saved at epoch 17 with validation loss 0.0158\n",
      "Epoch 18/20 - Train Loss: 0.0149 - Val Loss: 0.0149\n",
      "Best model saved at epoch 18 with validation loss 0.0149\n",
      "Epoch 19/20 - Train Loss: 0.0137 - Val Loss: 0.0139\n",
      "Best model saved at epoch 19 with validation loss 0.0139\n",
      "Epoch 20/20 - Train Loss: 0.0126 - Val Loss: 0.0130\n",
      "Best model saved at epoch 20 with validation loss 0.0130\n",
      "Final Test Loss: 0.0123\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"/Users/adepope/Documents/symbolic_py/data.prefix.expressions\"\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['f', 'taylor_f'], engine='python', nrows=10000)\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"])\n",
    "tokenizer.train_from_iterator(df[\"f\"].tolist() + df[\"taylor_f\"].tolist(), trainer)\n",
    "\n",
    "df[\"f_tokens\"] = df[\"f\"].apply(lambda x: encode_sequence(x, tokenizer))\n",
    "df[\"taylor_f_tokens\"] = df[\"taylor_f\"].apply(lambda x: encode_sequence(x, tokenizer))\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.stack(list(train_df[\"f_tokens\"])), \n",
    "    torch.stack(list(train_df[\"taylor_f_tokens\"]))\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.stack(list(val_df[\"f_tokens\"])), \n",
    "    torch.stack(list(val_df[\"taylor_f_tokens\"]))\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.stack(list(test_df[\"f_tokens\"])), \n",
    "    torch.stack(list(test_df[\"taylor_f_tokens\"]))\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = Seq2SeqTransformer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_model_path = \"best_transformer_model_taylor4.pth\"\n",
    "\n",
    "train_transformer(model, train_loader, val_loader, epochs, criterion, optimizer, device, best_model_path)\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_loss = evaluate_transformer(model, test_loader, criterion, device)\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Function: add x add pow x INT+ 2 mul x exp x\n",
      "Predicted Expression: add mul div INT + 1 INT + 2 pow x INT + 3 add mul INT + 2 x add mul INT + 2 pow x INT + 2 mul div INT + 1 INT + 6 pow x INT + 4\n",
      "Target Expression: add mul div INT+ 1 INT+ 2 pow x INT+ 3 add mul INT+ 2 x add mul INT+ 2 pow x INT+ 2 mul div INT+ 1 INT+ 6 pow x INT+ 4\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = Seq2SeqTransformer(vocab_size=tokenizer.get_vocab_size()).to(device)\n",
    "best_model_path = \"best_transformer_model_taylor4.pth\"\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "example_idx = 1\n",
    "input_function = test_df.iloc[example_idx][\"f\"]\n",
    "target_value = test_df.iloc[example_idx][\"taylor_f\"]\n",
    "\n",
    "encoded_input = encode_sequence(input_function, tokenizer)\n",
    "encoded_input = encoded_input.unsqueeze(0).to(device)\n",
    "encoded_target = encode_sequence(target_value, tokenizer).unsqueeze(0).to(device)\n",
    "\n",
    "tgt_input = encoded_target[:, :-1]\n",
    "tgt_output = encoded_target[:, 1:]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(encoded_input, tgt_input)\n",
    "\n",
    "# print(outputs)\n",
    "predicted_ids = outputs.argmax(dim=-1)[0].cpu().numpy()\n",
    "# print(predicted_ids)\n",
    "predicted_output = tokenizer.decode(predicted_ids)\n",
    "\n",
    "print(f\"Input Function: {input_function}\")\n",
    "# print(tokenizer.decode(encoded_target[:,0]))\n",
    "print(f\"Predicted Expression: {tokenizer.decode(encoded_target[:,0].cpu().numpy())}\" + \" \" +  predicted_output)\n",
    "print(f\"Target Expression: {target_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
